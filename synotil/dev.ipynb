{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is in ipython: 1\n",
      "is in ipython: 1\n",
      "[WARN] pymisca.vis_util cannot find network\n"
     ]
    }
   ],
   "source": [
    "import scipy.cluster.hierarchy as sphier\n",
    "import scipy.spatial.distance as spdist\n",
    "import pymisca.vis_util as pyvis\n",
    "import pymisca.util as pyutil\n",
    "plt = pyvis.plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spdist.squareform(np.ojobnes((3,3)),checks=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sphier\n",
    "import scipy.spatial.distance as spdist\n",
    "import pymisca.vis_util as pyvis\n",
    "import pymisca.util as pyutil\n",
    "plt = pyvis.plt\n",
    "\n",
    "# sphier.dendrogram??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat ~/bash.he"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting __init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile __init__.py\n",
    "import CountMatrix as scount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting norm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile norm.py\n",
    "import numpy as np\n",
    "import pymisca.util as pyutil\n",
    "\n",
    "import sklearn.decomposition as skdecomp\n",
    "pd = pyutil.pd\n",
    "import CountMatrix as scount\n",
    "# import util as util\n",
    "import sys\n",
    "import pymisca.ext\n",
    "snorm = sys.modules[__name__]\n",
    "\n",
    "\n",
    "def stdNorm(X,meanNorm=0,\n",
    "            normF = pyutil.l2norm):\n",
    "    deco = 0\n",
    "    if isinstance(X, scount.countMatrix):\n",
    "        deco =1\n",
    "        df = X \n",
    "    if meanNorm:\n",
    "        X = snorm.meanNorm(X)\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "    X = X.copy()\n",
    "    assert isinstance(X,np.ndarray)\n",
    "    \n",
    "    STD = normF(X,axis=1,keepdims=1); pos = np.squeeze(STD>0);\n",
    "    X[pos] = X[pos]/STD[pos]\n",
    "\n",
    "    if deco:\n",
    "        X = df.setDF(X)\n",
    "        X.param['normF'] = 'stdNorm'\n",
    "    return X\n",
    "\n",
    "def meanNorm(X,axis=1):\n",
    "    deco = 0\n",
    "    if isinstance(X,scount.countMatrix):\n",
    "        deco =1\n",
    "        df = X \n",
    "        \n",
    "    if isinstance(X,pd.Series):\n",
    "        X = X.values\n",
    "        X = (X - X.mean())\n",
    "    else:\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "            \n",
    "        assert isinstance(X,np.ndarray)\n",
    "        X = (X-X.mean(axis=axis,keepdims=1))\n",
    "        \n",
    "    param = getattr(X,'param',{})    \n",
    "    param['normF'] = 'meanNorm'\n",
    "#     X.param = param\n",
    "    \n",
    "    if deco:\n",
    "#         cols = X.columns\n",
    "        X = df.setDF(X)\n",
    "#         X.columns = cols\n",
    "        X.param['normF'] = 'meanNorm'        \n",
    "    return X\n",
    "\n",
    "def sumNorm(X,axis=1):\n",
    "    deco = 0\n",
    "    if isinstance(X,scount.countMatrix):\n",
    "        deco =1\n",
    "        df = X \n",
    "        \n",
    "    if isinstance(X,pd.Series):\n",
    "        X = X.values\n",
    "        SUM = X.sum()\n",
    "        if SUM != 0:\n",
    "            X = (X / SUM)\n",
    "    else:\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "            \n",
    "        assert isinstance(X,np.ndarray)\n",
    "        SUM =X.sum(axis=axis,keepdims=1)\n",
    "        SUM[SUM==0] = 1.\n",
    "        X = (X / SUM)\n",
    "        \n",
    "    if deco:\n",
    "#         cols = X.columns\n",
    "        X = df.setDF(X)\n",
    "#         X.columns = cols\n",
    "        X.param['normF'] = 'sumNorm'        \n",
    "    return X\n",
    "\n",
    "def diffNorm(X,axis=1):\n",
    "    deco = 0\n",
    "    if isinstance(X,scount.countMatrix):\n",
    "        deco =1\n",
    "        df = X \n",
    "        \n",
    "    if isinstance(X,pd.Series):        \n",
    "        X = X.values\n",
    "        X = np.diff(X,axis=0)\n",
    "    else:\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values            \n",
    "        assert isinstance(X,np.ndarray)\n",
    "        X = np.diff(X,axis=axis)\n",
    "#         X = (X-X.mean(axis=axis,keepdims=1))\n",
    "        \n",
    "    param = getattr(X,'param',{})    \n",
    "    param['normF'] = 'diffNorm'\n",
    "#     X.param = param\n",
    "    \n",
    "    if deco:\n",
    "#         cols = X.columns\n",
    "        X = df.setDF(X)\n",
    "#         X.columns = cols\n",
    "        X.param['normF'] = 'diffNorm'        \n",
    "    return X\n",
    "\n",
    "def meanNormProj(X):\n",
    "    deco = 0\n",
    "    if isinstance(X,scount.countMatrix):\n",
    "        deco =1\n",
    "        df = X\n",
    "        \n",
    "    X = meanNorm(X)\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values    \n",
    "        \n",
    "    D = X.shape[1]\n",
    "    Wn  = pyutil.meanNormBasis(D,orthonormal =1 )\n",
    "    X = X.dot(Wn.T)\n",
    "    \n",
    "    param = getattr(X,'param',{})    \n",
    "    param['normF'] = 'meanNormProj'\n",
    "#     X.param = param\n",
    "\n",
    "    if deco:\n",
    "        X = df.setDF(X)\n",
    "        X.param['normF'] = 'meanNormProj'        \n",
    "    return X\n",
    "import sklearn.decomposition as skdecomp\n",
    "def meanNormPCA(X,cutoff=1.0, withModel=0):\n",
    "    name = 'meanNormPCA'\n",
    "    deco = 0\n",
    "    if isinstance(X,scount.countMatrix):\n",
    "        deco =1\n",
    "        df = X\n",
    "        \n",
    "    X = meanNorm(X)\n",
    "    X = X.values if isinstance(X, pd.DataFrame) else X\n",
    "    \n",
    "    X0 = X\n",
    "    mdl = skdecomp.PCA()\n",
    "    mdl.fit(X)\n",
    "    \n",
    "    X = mdl.transform(X)\n",
    "    \n",
    "    first = np.cumsum(mdl.explained_variance_ratio_) >= cutoff\n",
    "    idx   =np.argmax(first) + 1\n",
    "    mdl.components_ = mdl.components_[:idx]\n",
    "#     mdl.mean_ = mdl.mean_[:,idx]\n",
    "\n",
    "    X = X[:,:idx]\n",
    "        \n",
    "    param = getattr(X,'param',{})    \n",
    "    param['normF'] = name\n",
    "#     X.param = param\n",
    "\n",
    "    if deco:\n",
    "        X = df.setDF(X)\n",
    "        X.param['normF'] = name     \n",
    "    if withModel:\n",
    "        return (X,mdl,X0)\n",
    "    else:\n",
    "        return X\n",
    "def ctNorm(X):\n",
    "    X = (X-X.mean(axis=1,keepdims=1))\n",
    "    return X\n",
    "def identityNorm(X):\n",
    "    param = getattr(X,'param',{})    \n",
    "    param['normF'] = 'identityNorm'\n",
    "#     X.param = param    \n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting qcmsg.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile qcmsg.py\n",
    "\n",
    "def msgGMM(model = None, train_data = None,name='test',**kwargs):\n",
    "    mdl = model\n",
    "    s = '''\n",
    "Name:{name}\n",
    "Converged:{cvg}\n",
    "min_logL: {logL}\n",
    "(lower-bound of) MEAN logL :{mlogL}'''.format(\n",
    "        name=name,\n",
    "         cvg=mdl.converged_,\n",
    "         logL = mdl.lower_bound_,\n",
    "         mlogL=mdl.lower_bound_/len(train_data) if not train_data is None else mdl.lower_bound_\n",
    "                                )\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modelRoutine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modelRoutine.py\n",
    "\n",
    "import sklearn.mixture as skmix\n",
    "import sklearn.decomposition.pca as skpca\n",
    "\n",
    "import pymisca.util as pyutil\n",
    "np = pyutil.np; pd = pyutil.pd\n",
    "import os\n",
    "import qcmsg\n",
    "import synotil.CountMatrix as scount\n",
    "# import synotil.qcplots as qcplots\n",
    "\n",
    "def fit_PCA(C,n_components=5,**kwargs):\n",
    "    mdl = skpca.PCA(n_components=n_components,**kwargs)\n",
    "    M = mdl.fit_transform(C)\n",
    "    \n",
    "    resDict = {'model':mdl,\n",
    "            'train_data':C,\n",
    "            'trans_data':M,}\n",
    "\n",
    "    return pyutil.util_obj(**resDict)\n",
    "\n",
    "\n",
    "def fit_BGM(C,\n",
    "            ALI = 'Test',\n",
    "#             DIR = ''\n",
    "#             normF = identityNorm,\n",
    "            stdPer = 0,\n",
    "            rowName=None,\n",
    "            colName=None,\n",
    "            nClu = 25,\n",
    "            maxIt = 1000,\n",
    "            algo = 'DPGMM',\n",
    "            DIR= '.',\n",
    "#            algoLst = ['DPGMM'],\n",
    "            alpha = .1,\n",
    "            covariance_type = 'diag',\n",
    "            fixMean = 0,\n",
    "            reorder=1,\n",
    "            model_only =0,\n",
    "            random_state= None,\n",
    "            dbg=0,\n",
    "#             covariance_type = None,\n",
    "#             **kwargs\n",
    "           ):\n",
    "    '''\n",
    "Fit an BayesianGaussianMixture() model from sklearn\n",
    "'''\n",
    "#     if algoLst is None:\n",
    "#         algoLst = ['DPGMM','DDGMM','GMM',]    \n",
    "    try:\n",
    "        DIR,ALI = ALI.rsplit('/',1)\n",
    "    except:\n",
    "        DIR= DIR\n",
    "    os.system('mkdir -p %s'%(DIR))\n",
    "    \n",
    "    \n",
    "    ###### Manage meta attributes of the model ########\n",
    "    param = {\n",
    "            'fixMean':fixMean,\n",
    "             'stdPer':stdPer,\n",
    "             'nClu':nClu,\n",
    "             'genre':algo,\n",
    "             'covarianceType': covariance_type,\n",
    "              'maxIt' : maxIt,\n",
    "              'randomState':random_state,\n",
    "         }\n",
    "    param.update(getattr(C,'param',{}))\n",
    "    \n",
    "    ####### Convert to numpy arrary ######\n",
    "    if isinstance(C,pd.DataFrame):\n",
    "        if  ALI=='Test':\n",
    "            ALI = getattr(C,'name','Test') \n",
    "\n",
    "        rowName,colName,C = C.index.values, C.columns, C.values\n",
    "        pass\n",
    "    \n",
    "    ##### Old routine that filter by STD ###########\n",
    "    if stdPer > 0 :\n",
    "        assert stdPer < 100,'Percentile must < 100, Got %d instead'%stdPer\n",
    "        (MEAN,STD,CV),_ = qc_Avg(C)\n",
    "        pIdx = STD > np.percentile(STD, stdPer)        \n",
    "        rowName = np.array(rowName)[pIdx]; C = C[pIdx]\n",
    "    print '[ALI]=',ALI\n",
    "    nFeat = C.shape[-1]\n",
    "        \n",
    "    #####====== Defnitions of fitters=========#######\n",
    "    \n",
    "    ###### Arguments shared among fitters ######\n",
    "    common = {'n_components': nClu,\n",
    "          'verbose':2,\n",
    "         'max_iter':maxIt,\n",
    "             'covariance_type':covariance_type,\n",
    "              'random_state':random_state,\n",
    "             }\n",
    "    if fixMean:\n",
    "        mean_precision_prior = 1E-128\n",
    "        mean_prior = [0.]*nFeat\n",
    "    else:\n",
    "        mean_precision_prior  = None\n",
    "        mean_prior = None\n",
    "        \n",
    "    ####### List of fitters ######\n",
    "    mdlLst = {'DPGMM':skmix.BayesianGaussianMixture(weight_concentration_prior_type='dirichlet_process',\n",
    "                                        weight_concentration_prior=alpha,\n",
    "                                        mean_precision_prior = mean_precision_prior,\n",
    "                                        mean_prior = mean_prior,\n",
    "                                       **common),\n",
    "          'GMM':skmix.GaussianMixture(**common),\n",
    "          'DDGMM':skmix.BayesianGaussianMixture(weight_concentration_prior_type='dirichlet_distribution',\n",
    "                                        weight_concentration_prior=alpha,\n",
    "                                        mean_precision_prior = mean_precision_prior,\n",
    "                                        mean_prior = mean_prior,\n",
    "                                       **common),\n",
    "         }\n",
    "    \n",
    "    \n",
    "    ############# Select model by \"algo\"####\n",
    "    X = C\n",
    "#     if dbg >= 2:\n",
    "#         qcplots.qc_Avg(C,silent=0)\n",
    "    print pyutil.qc_matrix(X)\n",
    "    mdl = mdlLst.get(algo,None)\n",
    "    assert mdl is not None, 'Algorithm %s not found '%algo\n",
    "    \n",
    "    NAME = '%s_%s'%(ALI,pyutil.dict2flat(param))    \n",
    "    print '[MSG] Now Fitting Model:%s'%NAME\n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Meta data of the training Data #######\n",
    "    d = {'name': NAME,\n",
    "         'train_data':X,\n",
    "         'colName':colName,\n",
    "         'rowName':rowName,\n",
    "         'param':param,\n",
    "       }\n",
    "    \n",
    "    \n",
    "    ##### Fitting model and caching the result to specified DIR/NAME ####\n",
    "    try:\n",
    "        logFile = open('%s/%s.log'%(DIR,NAME),'w',0)\n",
    "        with pyutil.RedirectStdStreams(logFile):\n",
    "            mdl.fixMean= fixMean\n",
    "            mdl.fit(X)\n",
    "#             reorderByMSQ(mdl)\n",
    "            if reorder:\n",
    "                mdl.reorderByMSQ()\n",
    "            d.update({'suc':1,'model':mdl})\n",
    "#             logFile.close()\n",
    "        print \"[SUCC] to fit Model:%s\"%(NAME,)\n",
    "        print qcmsg.msgGMM(mdl)\n",
    "    except Exception as e:\n",
    "        print \"[FAIL] to fit Model:%s due to :'%s'\"%(NAME,e)\n",
    "        d.update({'suc':0})\n",
    "    if model_only:\n",
    "        d['train_data'] = None\n",
    "        d['rowName'] = None\n",
    "        d['colName'] = None\n",
    "    np.save('%s/%s'%(DIR.rstrip('/'),NAME),d)\n",
    "    d = scount.countMatrix.from_dict(d)\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "is in ipython: 1 \n",
      "[WARN]No module named jinja2_util\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] pymisca.vis_util cannot find network\n",
      "[WARN] pymisca.vis_util cannot find network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "is in ipython: 1 \n",
      "[WARN]No module named jinja2_util\n"
     ]
    }
   ],
   "source": [
    "import pymisca.vis_util as pyvis\n",
    "import pymisca.util as pyutil\n",
    "plt = pyvis.plt\n",
    "import synotil.modelRoutine as smod\n",
    "import synotil.CountMatrix as scount\n",
    "import synotil.qcplots as sqc\n",
    "import synotil.dio as sdio\n",
    "import synotil.norm as snorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymisca.tensorflow_extra_.hyper_plane_mixture as hpm\n",
    "# hpm.main._fit_MAP??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting jobs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile jobs.py\n",
    "# import sy\n",
    "# import scipy.cluster.hierarchy as sphier\n",
    "# import scipy.spatial.distance as spdist\n",
    "import pymisca.vis_util as pyvis\n",
    "import pymisca.util as pyutil\n",
    "plt = pyvis.plt; pd = pyutil.pd; np = pyutil.np\n",
    "import pymisca.models as pymod\n",
    "import pymisca.ext as pyext\n",
    "\n",
    "import synotil.modelRoutine as smod\n",
    "import synotil.CountMatrix as scount\n",
    "import synotil.PanelPlot as spanel\n",
    "import synotil.qcplots as sqc\n",
    "import synotil.dio as sdio\n",
    "job__nearAUG = sdio.job__nearAUG\n",
    "\n",
    "import synotil.norm as snorm\n",
    "sjob= pyutil.sys.modules[__name__]\n",
    "\n",
    "\n",
    "def figs__peakBW(peakFile,\n",
    "                 bwFiles,\n",
    "                 outerRadius = 500,\n",
    "                 innerRadius=50, \n",
    "                 stepSize=10,\n",
    "                 center_summit = 0,\n",
    "                 outIndexFunc=None,\n",
    "                 outIndex=pyutil.basename,\n",
    "                 ylim = [0,None],\n",
    "                 NCORE=4,\n",
    "                 squareSize=(0.2,0.01),\n",
    "                 detailByChip = None,\n",
    "                 detailByGene = 0, ### runtime linear in number of genes\n",
    "                 name = None,\n",
    "                 **kwargs):\n",
    "    ### legacy\n",
    "    if outIndexFunc is not None:\n",
    "        assert outIndex is None\n",
    "        outIndex= outIndexFunc\n",
    "    else:\n",
    "        assert outIndex is not None\n",
    "        pass\n",
    "    #### get the data ready\n",
    "    bwTracks = sdio.extract_bigwig_multiple(bedFile=peakFile,fnames=bwFiles,\n",
    "                                            radius=outerRadius,NCORE=NCORE,\n",
    "                                             stepSize=stepSize,\n",
    "                                             outIndex=outIndex,\n",
    "                                            center_summit = center_summit,\n",
    "                                            **kwargs)\n",
    "    if detailByChip is None:\n",
    "        if len(bwTracks) <= 100:\n",
    "            detailByChip = 1\n",
    "        else:\n",
    "            detailByChip = 0\n",
    "            \n",
    "    if name is None:\n",
    "        name = pyutil.getBname(peakFile)\n",
    "        \n",
    "    poss = bwTracks.columns.levels[1]\n",
    "    innerPos = poss[abs(poss) <= innerRadius]\n",
    "\n",
    "    bwAvg = pyutil.colGroupMean(bwTracks.reindex(columns=innerPos,level=1))\n",
    "    bwAvg = scount.countMatrix(bwAvg).apply(pyutil.log2p1)\n",
    "\n",
    "    ##### plotting\n",
    "#     fig,axs = plt.subplots(1,2,figsize=[12,6])\n",
    "#     figs = {}\n",
    "    figs = pyutil.collections.OrderedDict()\n",
    "    #########\n",
    "#     plt.figure(figsize=[6,4])\n",
    "    fig,axs = plt.subplots(1,3,figsize=[18,6])\n",
    "\n",
    "    ax =axs[0]\n",
    "    bwAvg.boxplot(rot='vertical',ax=ax)\n",
    "    ax.set_ylabel('log2(peak intensity)')\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    ax =axs[1]\n",
    "    sqc.qc_pileUp(bwTracks,ax=ax,axLeg=axs[2])\n",
    "    figs['pileUp-%s' % name ]  = plt.gcf()\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "    #########\n",
    "    bwAvg.heatmap(figsize=[20,10])\n",
    "    figs['avgHeatmap-%s' % name] =plt.gcf()\n",
    "    \n",
    "    pos = bwTracks.columns.get_level_values('pos')\n",
    "    cname = 'binding'\n",
    "    #########\n",
    "    if detailByChip:\n",
    "        for key,bwTrack in bwTracks.groupby(axis=1,level=0):\n",
    "            dfc = bwTrack\n",
    "            pyvis.heatmap(dfc,\n",
    "                          transpose=0,\n",
    "                          squareSize=(0.025,0.2),\n",
    "                          ytick = dfc.index,\n",
    "                          xlab = 'distance to %s' % key,\n",
    "                         vlim=ylim,\n",
    "                         cname=cname)\n",
    "            \n",
    "            ax = plt.gca()\n",
    "            xticks = pos[ax.get_xticks().astype(int)[:-1]]\n",
    "            ax.set_xticklabels(xticks,)\n",
    "\n",
    "#             pyvis.heatmap(bwTrack,\n",
    "#                           transpose=1,\n",
    "#                           squareSize=squareSize,\n",
    "#                           xtick = bwTracks.index,ylab = key)\n",
    "            figs['detailByChip-%s/%s'%(name,key)] = plt.gcf()\n",
    "    \n",
    "    ########\n",
    "    if detailByGene:\n",
    "\n",
    "        for key,bwTrack in bwTracks.groupby(axis=0,level=0):\n",
    "            dfc = bwTrack.melt().pivot_table(index='bwFile',\n",
    "                                                 columns='pos',)\n",
    "            pyvis.heatmap(dfc,\n",
    "                          transpose=0,\n",
    "                          squareSize=(0.025,0.2),\n",
    "                          ytick = dfc.index,\n",
    "#                           xtick = dfc.columns,                          \n",
    "                          xlab = 'distance to %s' % key,\n",
    "                          vlim=ylim,\n",
    "                         cname = cname)\n",
    "            ax = plt.gca()\n",
    "            xticks = pos[ax.get_xticks().astype(int)[:-1]]\n",
    "            ax.set_xticklabels(xticks,)\n",
    "            \n",
    "            figs['detailByGene-%s/%s'%(name,key)] = plt.gcf()\n",
    "            \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    return figs,(bwTracks,bwAvg)\n",
    "\n",
    "panel_kw_dft =  dict(\n",
    "    figsize=[14, 7],\n",
    "    show_axa = 1,\n",
    "    show_axc = 0,\n",
    "    showGrid = 0,\n",
    "    width_ratios = [1,14,0.]\n",
    "#     title= '',\n",
    "#     height_ratios = [1,3,3,3,1],\n",
    "    )\n",
    "def job__render__panelPlot(\n",
    "    tracks = None,\n",
    "    clu=None,\n",
    "    order=None,\n",
    "    index=None,\n",
    "    aliasFmt = '{alias}',\n",
    "    alias = None,\n",
    "    baseFile=0,\n",
    "    figsize = None,\n",
    "    panel_kw = panel_kw_dft,\n",
    "    how = 'left',\n",
    "    debug=0,\n",
    "    extra = {},\n",
    "    **kwargs\n",
    "):\n",
    "    if figsize is not None:\n",
    "        panel_kw['figsize'] = figsize\n",
    "    autoAli = alias is None\n",
    "    if autoAli:\n",
    "        alias = ''\n",
    "    if isinstance(clu,basestring):\n",
    "        alias += pyext.getBname(clu)\n",
    "        clu = pyutil.readData(clu,baseFile=baseFile)\n",
    "    if isinstance(order,basestring):\n",
    "        alias += pyext.getBname(order)\n",
    "        order = pyutil.readData(order,baseFile=baseFile)\n",
    "    if isinstance(tracks,basestring):\n",
    "        alias += pyext.getBname(tracks)\n",
    "        tracks = pyutil.readData(tracks,baseFile=baseFile)\n",
    "        tracks = list(tracks)\n",
    "    if isinstance(panel_kw,basestring):\n",
    "        alias += pyext.getBname(panel_kw)\n",
    "        panel_kw = pyutil.read__buffer( panel_kw,\n",
    "                                       ext='json',typ='rec',\n",
    "                                       guess_index=0).to_dict()\n",
    "    if order is not None:\n",
    "        clu = order.get(['clu'])\n",
    "    else:\n",
    "        assert clu is not None\n",
    "        order = pd.DataFrame(clu)\n",
    "        \n",
    "    if isinstance(index,basestring):\n",
    "        alias += pyutil.sanitise_query(index)\n",
    "        locals().update(extra)\n",
    "        index = eval(index)\n",
    "\n",
    "    cluTrack = spanel.fixCluster(clu.get(['clu']))\n",
    "    alias = aliasFmt.format(**locals())\n",
    "#     cluFile_clean = 'clean_%s.csv' % alias\n",
    "#     cluc.to_csv(cluFile_clean)    \n",
    "    tracks = pyext.list__realise(tracks, locals())\n",
    "    ##### Output heatmap\n",
    "    pp = spanel.panelPlot(tracks,**panel_kw)\n",
    "    pp.compile(how=how,\n",
    "               index=index,\n",
    "               **kwargs\n",
    "              )\n",
    "    pp.compile(order=order)\n",
    "#     if debug:\n",
    "#         return pp\n",
    "    if debug:\n",
    "        return pp\n",
    "    fig = pp.render();    \n",
    "    return (alias,fig)\n",
    "\n",
    "def job__rawFile__combine(dfc,silent=0):\n",
    "    '''take a dataframe that identifies raw files and\n",
    "    convert it to combined fastq.gz ready for downstream.\n",
    "'''\n",
    "    dfc = sdio.rawFile__validateChunk(dfc)\n",
    "    dfcCombined = dfc.groupby('fnameCombined',\n",
    "                              as_index=False\n",
    "                             ).apply(sdio.rawFile__combineChunk,\n",
    "                                            silent=0)\n",
    "    return dfcCombined\n",
    "\n",
    "\n",
    "def worker__md5sum(row,column='fname'):\n",
    "    checkSum_col = '%s_md5sum' % column\n",
    "    dct= row\n",
    "    val = dct[column]\n",
    "    res = pyutil.getMD5sum(val)\n",
    "    dct[checkSum_col] = res\n",
    "    return dct\n",
    "\n",
    "def job__md5sum(dfc,column='fname',n_cpu=1, **kwargs):\n",
    "    worker = pyutil.functools.partial(worker__md5sum,\n",
    "                                     column=column)\n",
    "    it = (x.__dict__ for x in dfc.itertuples())\n",
    "    res = pyutil.mp_map(worker, it ,n_cpu = n_cpu, **kwargs)\n",
    "#     res = pd.DataFrame(res)\n",
    "    return res\n",
    "\n",
    "def fig__fluffProfile(interval,tracks,ofname = None,\n",
    "                      annotation= None,\n",
    "                      scaleOpt = None, \n",
    "                      fragmentSize=0,\n",
    "                      labels = None,\n",
    "                        silent = 0):\n",
    "#     annotation = BED12\n",
    "    # cmd = 'fluff profile'\n",
    "    trackFlat = u' '.join(tracks)\n",
    "    if scaleOpt is None:\n",
    "        scaleOpt = ' -s 1:%d ' % (len(tracks))\n",
    "    if ofname is None:\n",
    "        ofname = interval + '.svg'\n",
    "    cmd =  ''\n",
    "    cmd += ' fluff profile '\n",
    "    cmd +=  scaleOpt\n",
    "    cmd += ' -f {fragmentSize} '\n",
    "    if annotation is not None:\n",
    "        cmd += ' -a {annotation} '\n",
    "    if labels is not None:\n",
    "        labelFlat = u' '.join(labels)\n",
    "        cmd += ' -l {labelFlat}'\n",
    "    cmd += ' -o {ofname} -i {interval} -d {trackFlat} '\n",
    "    cmd += ' -b white '\n",
    "    cmd += ' 2>&1 '\n",
    "    cmd = cmd.format(**locals())\n",
    "    res=  pyutil.shellexec(cmd,silent=silent)\n",
    "    return ofname\n",
    "\n",
    "fig__fluffyProfile = fig__fluffProfile ###legacy\n",
    "\n",
    "\n",
    "def job__cluster__hpm(tdf,name='test0', \n",
    "                      K =40 ,meanNorm=1,\n",
    "                      threshold=0.,batchSize=500,n_iter=3000,\n",
    "                      silent=0,\n",
    "                      NCORE=4,\n",
    "                      randomState=0,\n",
    "                      alpha = None,\n",
    "                      weighted = True,\n",
    "                     ):\n",
    "    import pymisca.tensorflow_extra_.hyper_plane_mixture as hpm\n",
    "    hpm.tf.set_random_seed(randomState)\n",
    "    np.random.seed(randomState)\n",
    "    mdl =  hpm.main(K=K,NCORE=NCORE,name=name,meanNorm=meanNorm,threshold=threshold,\n",
    "                   weighted=weighted,alpha = alpha)\n",
    "    if batchSize==0 or batchSize is None:\n",
    "        batchMaker = None\n",
    "#         batchMaker = hpm.pytfu.batchMaker__random(batchSize=batchSize)\n",
    "    else:\n",
    "         batchMaker = hpm.pytfu.batchMaker__random(batchSize=batchSize)\n",
    "    res = mdl.fit(tdf,\n",
    "            batchMaker = batchMaker,\n",
    "            n_iter=n_iter,autoStop=0,\n",
    "    )\n",
    "    if not silent:\n",
    "#         import matplotlib.pyplot as plt\n",
    "        plt.plot(res)\n",
    "    cdict = pymod.cache__model4data(mdl,tdf) \n",
    "#     assert 0\n",
    "    mdl.post.__dict__.update(cdict)\n",
    "    np.save('params.npy', mdl.params)\n",
    "    res = mdl.params\n",
    "    res['mdl'] = mdl\n",
    "    return pyutil.util_obj(**res)\n",
    "\n",
    "def job__cluster__vmf(tdf,\n",
    "                      K = 30, init_method='kmeans',\n",
    "                      weighted= True,\n",
    "                      n_iter=3000,randomState=None,\n",
    "                    nStart = 15,\n",
    "                      min_iters = 50,\n",
    "                    verbose=1,\n",
    "                    callback= None,\n",
    "                    silent=0,\n",
    "                      sample_weights= 'sd',\n",
    "):\n",
    "    import pymisca.model_collection.mixture_vmf as mod    \n",
    "    np.random.seed(randomState)\n",
    "    mdl = mod.MixtureVMF(K=K,init_method=init_method,\n",
    "                         weighted = weighted,\n",
    "                        )\n",
    "\n",
    "    histLoss = mdl.fit(tdf,\n",
    "                       verbose=verbose,\n",
    "                       callback=callback,\n",
    "                       nStart=nStart,\n",
    "                       n_iter = n_iter,\n",
    "                       min_iters = min_iters,\n",
    "                       sample_weights =sample_weights,\n",
    "                      )\n",
    "    histLoss = -histLoss\n",
    "    \n",
    "    if not silent:\n",
    "#         import matplotlib.pyplot as plt\n",
    "        plt.plot(histLoss)    \n",
    "    cdict = pymod.cache__model4data(mdl,tdf) \n",
    "    cdict.update(mdl.params)\n",
    "\n",
    "    np.save('params.npy', cdict)\n",
    "\n",
    "    cdict['mdl'] = mdl\n",
    "    return pyutil.util_obj(**cdict)\n",
    "\n",
    "def job__combinePeak(bwCurr,\n",
    "                     featSummit='/home/feng/ref/Arabidopsis_thaliana_TAIR10/annotation/genes.gtf.cds.summit',\n",
    "                     GSIZE='/home/feng/ref/Arabidopsis_thaliana_TAIR10/genome.sizes',\n",
    "                    CUTOFF=4000,\n",
    "                     head = 1000,\n",
    "                    alias = 'testPeaks',\n",
    "                    center_summit = 1,):\n",
    "\n",
    "    bwCurr = bwCurr.dropna(subset=['npkFile'])\n",
    "    bwCurr['npkFileLine'] = bwCurr.eval(\"npkFile.map(@pyutil.lineCount)\")\n",
    "    print (bwCurr[['bname','npkFileLine']].sort_values('bname'))\n",
    "\n",
    "    dfs = zip(bwCurr.index,map(sdio.extract_peak,bwCurr.npkFile))\n",
    "    dfs = dict(dfs)\n",
    "\n",
    "    dfs = {k:v.sort_values('score',ascending=False) for k,v in dfs.items()}\n",
    "    bed = pd.concat([df.head(head) for df in dfs.values()],axis=0,sort=False)\n",
    "    bed = bed.dropna(axis=1)\n",
    "\n",
    "    ofname = pyutil.to_tsv( bed,'%s__combined.bed' % alias)\n",
    "    if (len(dfs) > 1) and center_summit :\n",
    "        ofname = sdio.bed__merge(ofname,silent=0)\n",
    "    # pyutil.print\n",
    "    print(ofname,pyutil.lineCount(ofname))\n",
    "\n",
    "    bedFile= ofname\n",
    "    \n",
    "#     peakSummit = sdio.bed__summit(bedFile)\n",
    "    peakSummit = sdio.npk_expandSummit(fname=bedFile,radius=1,\n",
    "                                       center_summit=center_summit)\n",
    "    peak2geneFile = sdio.job__nearAUG(featSummit=featSummit,\n",
    "                                       peakSummit=peakSummit,\n",
    "                                       GSIZE=GSIZE,peakWid=1,\n",
    "                                       CUTOFF=CUTOFF)\n",
    "\n",
    "    pyutil.fileDict__save('files.json',\n",
    "                          d=locals(),\n",
    "                          keys= ['bedFile','peakSummit','peak2geneFile'])\n",
    "    return 'files.json'\n",
    "\n",
    "\n",
    "def job__chipTargPaired(\n",
    "    bwCurr  = None,\n",
    "    bwMeta = None, control= None, treatment = None,\n",
    "    xlab = None, ylab = None,\n",
    "    name = None,\n",
    "#     bwMeta,\n",
    "    NCORE = 2,\n",
    "    params__peakBW = None,\n",
    "    CUTOFF_FC = 3.0,\n",
    "    CUTOFF_CHIPDIFF = 0.7,\n",
    "    innerRadius = 100,\n",
    "):\n",
    "    figs = pyutil.collections.OrderedDict()\n",
    "\n",
    "    \n",
    "    if control is not None and treatment is not None:\n",
    "        xlab,ylab = control,treatment\n",
    "    if xlab is None or ylab is None:\n",
    "        xlab,ylab = bwCurr.index\n",
    "    elif bwCurr is None:\n",
    "        bwCurr = bwMeta.reindex([xlab,ylab])\n",
    "        \n",
    "    if params__peakBW is None:\n",
    "        \n",
    "        params__peakBW = dict(\n",
    "            outerRadius=500,\n",
    "            innerRadius=innerRadius,\n",
    "            NCORE = NCORE,\n",
    "            outIndex=bwCurr.header,\n",
    "        #     detailByCHIP = 0,\n",
    "        )\n",
    "    params__peakBW['innerRadius'] = innerRadius        \n",
    "        \n",
    "    if name is None:\n",
    "        name = '{xlab}-{ylab}'.format(**locals())\n",
    "#     bwCurr = bwMeta\n",
    "#     bwCurr = bwCurr.loc[[xlab,ylab]]\n",
    "\n",
    "#     bwCurr.npkFile\n",
    "\n",
    "    dfs = map(sdio.extract_peak, bwCurr.npkFile,)\n",
    "    \n",
    "    fig,ax = plt.subplots(1,1,figsize=[7,7])\n",
    "#     ax = plt.gca()\n",
    "    for df in dfs:\n",
    "        df['per_FC']  = pyutil.dist2ppf(df.FC)\n",
    "        df.plot.scatter('per_FC','FC',ax=ax)\n",
    "\n",
    "    fnames = [ pyutil.queryCopy(infile=fname,\n",
    "                                query='FC>%.3f'%CUTOFF_FC ,\n",
    "                                reader=sdio.extract_peak,inplace=False) \n",
    "        for fname in bwCurr.npkFile]\n",
    "    # dfs[1]\n",
    "\n",
    "    peakFlat = ' '.join(fnames)\n",
    "    ofname = '%s-combined.bed' % ('-'.join(bwCurr.index))\n",
    "    pyutil.shellexec('cat {peakFlat}>{ofname}'.format(**locals()))\n",
    "    ofname = sdio.npk_expandSummit(fname=ofname, radius=1)\n",
    "\n",
    "    pyutil.lineCount(ofname)\n",
    "    peakFileOrig = peakFile = ofname\n",
    "\n",
    "\n",
    "    res = sjob.figs__peakBW(\n",
    "        peakFile = peakFile,\n",
    "        bwFiles = bwCurr.RPKMFile,\n",
    "        name = name,\n",
    "        **params__peakBW\n",
    "    )\n",
    "    figs.update(res[0])\n",
    "\n",
    "    bwTrack,bwAvg = res[1]\n",
    "    bwAvg.columns = bwCurr.index\n",
    "\n",
    "    xs,ys = bwAvg[[xlab,ylab]].values.T\n",
    "#     clu = None\n",
    "    query = ' val_{ylab} - val_{xlab} > {CUTOFF_CHIPDIFF} '.format(**locals())\n",
    "    qsans = pyutil.sanitise_query(query)\n",
    "    \n",
    "#     peakIndex = pyutil.df__pad(bwAvg).query(query).index\n",
    "    clu = pd.DataFrame(pyutil.df__pad(bwAvg))\n",
    "    peakIndex = clu.query(query).index\n",
    "    clu['clu']= clu.eval('index in @peakIndex')\n",
    "    \n",
    "    pyvis.qc_2var(xs,ys,clu=clu.clu,xlab=xlab,ylab=ylab)\n",
    "    figs['scatterPlot__%s' % name ]= plt.gcf()\n",
    "    cluFile = ofname = qsans+'.csv'\n",
    "    clu.to_csv(ofname)\n",
    "    print (ofname,pyutil.lineCount(ofname))\n",
    "    peakBase = pyutil.getBname(peakFile)\n",
    "    ofname =  '{peakBase}-{qsans}.bed'.format(**locals())\n",
    "    peakFile = pyutil.to_tsv(\n",
    "        sdio.extract_peak(peakFile).set_index('acc',drop=0).reindex(peakIndex),\n",
    "                           ofname)\n",
    "    pyutil.shellexec('mkdir -p output/')\n",
    "    pyutil.file__link(ofname,'output/%s.bed' % name,force=True)\n",
    "    \n",
    "#     peakFile = pyutil.queryCopy(peakFile,\n",
    "#                                 query='acc in @peakIndex',\n",
    "#                                 reader=sdio.extract_peak, \n",
    "#                                 peakIndex=peakIndex,\n",
    "#                                )\n",
    "#     peakFile =  '{peakFile}-{qsans}.bed'\n",
    "#     pyutil.fileDict__main(ofname='FILE.json',\n",
    "#                          **pyutil.dictFilter(locals(),\n",
    "#                                              keys=['cluFile','peakFile',\n",
    "#                                             'peakFileOrig']\n",
    "#                                             ))\n",
    "    \n",
    "    pyutil.fileDict__save(d=locals(), keys=['cluFile','peakFile',\n",
    "                                            'peakFileOrig'],fname='FILE.json')\n",
    "    return figs,clu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sjob.figs__peakBW??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "??pyutil.fileDict__save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] NUMPY is not limited cuz NCORE is not set\n",
      "[WARN] NUMPY is not limited cuz NCORE is not set\n",
      "[WARN] pymisca.vis_util cannot find network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "is in ipython: 1 \n",
      "[WARN]No module named jinja2_util\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] pymisca.vis_util cannot find network\n",
      "[WARN] Cannot find file:key.gene\n",
      "[WARN] Cannot find file:key.gene\n",
      "[WARN] pymisca.vis_util cannot find network\n",
      "[WARN] pymisca.vis_util cannot find network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "is in ipython: 1 \n",
      "[WARN]No module named jinja2_util\n",
      "is in ipython: 1 \n",
      "[WARN]No module named jinja2_util\n",
      "is in ipython: 1 \n",
      "[WARN]No module named jinja2_util\n",
      "/home/feng/.local/lib/python2.7/site-packages/pandas/core/computation/check.py:17: UserWarning:\n",
      "\n",
      "The installed version of numexpr 2.4.3 is not supported in pandas and will be not be used\n",
      "The minimum supported version is 2.4.6\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "execfile('/home/feng/meta/header_0903.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev.ipynb:1806:    \"def shot2html(indexFile):\\n\",\r\n",
      "shot2html.py:4:def shot2html(indexFile):\r\n"
     ]
    }
   ],
   "source": [
    "! grep -rnw *ipynb *py -e \"def shot2html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /home/feng/repos/BrachyPhoton/synotil/shot2html.py\n",
    "import synotil.dio as sdio\n",
    "# import PIL\n",
    "# import cairosvg\n",
    "# import pymisca.util as pyutil\n",
    "import os\n",
    "def make_thumbnail(path, size):\n",
    "#     newpath = os.path.dirname(path)\n",
    "#     image_name = os.path.basename(os.path.splitext(path)[0])\n",
    "#     newpath = newpath + '/' + image_name + '.thumbnail.jpg'\n",
    "\n",
    "\n",
    "    try:\n",
    "        sp = path.rsplit('.',1)\n",
    "        if len(sp)==1:\n",
    "            ext = ''\n",
    "            base = sp[0]\n",
    "        else:\n",
    "            base,ext = sp\n",
    "        if ext.lower()=='svg':\n",
    "            base = path.rsplit('.',1)[0]\n",
    "            with open(path,'r') as f:\n",
    "                svg = f.read()\n",
    "            cairosvg.svg2png(bytestring=svg,write_to=base + '.png')\n",
    "            path = base + '.png'\n",
    "            ext = 'png'\n",
    "\n",
    "        newpath =  base + '__thumbnail.' + ext \n",
    "        img = PIL.Image.open(path)\n",
    "        img.thumbnail(size)\n",
    "        img.save(newpath, None)\n",
    "\n",
    "    except Exception as e:\n",
    "        newpath = path\n",
    "        print('[WARN]image error for PATH %s:\\n%s'%(newpath,e))\n",
    "    return newpath\n",
    "\n",
    "\n",
    "def shot2html(indexFile,localPath = True):\n",
    "# indexFile = \"PROG=chipShots_bedFile=64-SD-ZT20-ELF3OX-RERUN_S8_peaks.narrowPeak.FC-GT-3dot0/index.tsv\"\n",
    "    dfc = sdio.extract_peak(indexFile)\n",
    "    dfc['img'] = dfc[dfc.columns[-1]]\n",
    "#     THUMBNAIL_SIZE = 256, 256\n",
    "    # HTML_REPORT_NAME='test.html'\n",
    "    ofname = HTML_REPORT_NAME='%s.html'%indexFile \n",
    "    TITLE = os.path.basename(indexFile)\n",
    "    # !pip install --user cairosvg==1.0.22\n",
    "    # cairosvg.svg2png?\n",
    "    # from cairosvg import svg2png\n",
    "\n",
    "    # generate an html page\n",
    "\n",
    "    fhtml = open(HTML_REPORT_NAME, 'w')\n",
    "#     TITLE = 'test'\n",
    "    fhtml.write('''\n",
    "    <html>\n",
    "    <head>\n",
    "    <style> \n",
    "        body {{font-family:\\\"HelveticaNeue-Light\\\", \\\"Helvetica Neue Light\\\", \\\"Helvetica neue\\\"}}\n",
    "    </style>\n",
    "    <title>IGV screenshots for {TITLE}</title>\n",
    "    '''.format(**locals()))\n",
    "\n",
    "    js = \"\"\"\n",
    "    <script>\n",
    "    //var frame = window.frames[\"postFrame\"];\n",
    "    //var frame = document.getElementsByName(\"postFrame\")[0];\n",
    "\n",
    "    //dir = document.URL.replace(/^.*[\\\\\\/]/, '')\n",
    "    //var frame = window.open(\"about:blank\",\"postFrame\", 'height=440,width=260,scrollbars=yes');\n",
    "    console.log(document.URL);\n",
    "\n",
    "\n",
    "    // Pass the checkbox name to the function\n",
    "    function checkbox() {\n",
    "       // var frame = window.frames[\"postFrame\"];\n",
    "    //    var doc = frame.document;\n",
    "    //  var doc = frame.contentDocument || frame.contentWindow.document\n",
    "\n",
    "      //opened.open();\n",
    "      var checkboxes = document.getElementsByName(\"line\");\n",
    "      var checkboxesChecked = [];\n",
    "\n",
    "      // loop over them all\n",
    "      html_string = '';\n",
    "      for (var i=0; i<checkboxes.length; i++) {\n",
    "         // And stick the checked ones onto an array...\n",
    "         if (checkboxes[i].checked) {\n",
    "            checkboxesChecked.push(checkboxes[i]);\n",
    "            html_string += checkboxes[i].value + '<br>';\n",
    "    //        doc.writeln(checkboxes[i].value);\n",
    "    //        doc.write(\"<br>\");\n",
    "         }\n",
    "      }\n",
    "      src = \"data:text/html;charset=utf-8,\" + escape(html_string);\n",
    "       var frame = window.open(src,\"postFrame\", 'height=440,width=260,scrollbars=yes');\n",
    "\n",
    "      //opened.close()\n",
    "      // Return the array if it is non-empty, or null\n",
    "      return checkboxesChecked.length > 0 ? checkboxesChecked : null;\n",
    "    }\n",
    "    </script>\n",
    "    </head>\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    fhtml.write(js)\n",
    "\n",
    "\n",
    "\n",
    "    fhtml.write('<body>\\n')\n",
    "\n",
    "    # fc = []\n",
    "    # for t in image_lst:\n",
    "    #     narrowpeak_line = t[0]\n",
    "    #     lst = narrowpeak_line.split()\n",
    "    #     fc.append(float(lst[4]))\n",
    "\n",
    "    # max_fc = max(fc)\n",
    "    # min_fc = min(fc)\n",
    "    # lmd = 0.8\n",
    "    # show_color_val = (1.0 - lmd) * min_fc +  1.0 * lmd * max_fc\n",
    "\n",
    "\n",
    "    colors = ['#ff9999','#ff8080','#ff6666','#ff4d4d','#ff3333','#ff1a1a','#ff0000']\n",
    "    sz_color = len(colors)\n",
    "\n",
    "    fhtml.write('<p><b>%s</b></p>' % ('&nbsp; &nbsp;'.join(['Peak_position','Peak_name', 'Integer_score_for_display', 'Dot','fold-change',' -log10pvalue',' -log10qvalue','Relative_summit_position_to_peak_start']))) # head\n",
    "\n",
    "    fhtml.write('<iframe name=\"postFrame\"></iframe>')\n",
    "    fhtml.write('<form>')\n",
    "    fhtml.write('<input type=\\\"submit\\\" value=\\\"Save\\\" size=\\\"35\\\" onClick=\\\"return checkbox();\\\">\\n')\n",
    "\n",
    "    for i in range(len(dfc)):\n",
    "        row = dfc.iloc[i]\n",
    "    #     if np.isnan(row.img):\n",
    "\n",
    "        if isinstance(row.img,float):\n",
    "            imgFile = './none'\n",
    "        else:\n",
    "            imgFile = row.img\n",
    "            \n",
    "        if localPath:\n",
    "            imgFile = './%s'% os.path.basename(imgFile)\n",
    "        else:\n",
    "            imgFile = '/notebooks%s' % imgFile            \n",
    "            \n",
    "        bedLine = ('\\t').join(map(str,row[:-1]))\n",
    "    #     bedLine = '\\t'.join(row)\n",
    "    #     line = line_lst[i]\n",
    "        ind = i\n",
    "        fhtml.write('<input type=\\\"checkbox\\\" id=\\\"%s\\\" name=\\\"line\\\" value=\\\"%s\\\" >' % (ind, \n",
    "                                                                                         bedLine.replace('\\t','\\t')\n",
    "                                                                                        ))\n",
    "        c = colors[0]\n",
    "        fhtml.write('<font color=%s>%s</font><br/>\\n' % (c,bedLine))\n",
    "        fhtml.write('<font color=%s>%s</font><br/>\\n' % (c, bedLine.replace('\\t','&emsp; ')))\n",
    "    #     timgFile = make_thumbnail(imgFile, THUMBNAIL_SIZE)\n",
    "    #     if JUPYTER:\n",
    "\n",
    "    \n",
    "    #     timgFile = '/notebooks' + timgFile\n",
    "    #     if imgFile.endswith('.svg'):\n",
    "    #         imgFile = imgFile.replace('.svg','.png')\n",
    "\n",
    "        fhtml.write('<a href=\\\"%s\\\">\\n<img src=\\\"%s\\\"></a><br/><br/>\\n' % (imgFile, imgFile))\n",
    "\n",
    "    fhtml.write('</form>')    \n",
    "    fhtml.write('</body>\\n')\n",
    "    fhtml.write('</html>\\n')\n",
    "    fhtml.close()    \n",
    "\n",
    "    # os.remove(PYTHON_SCRIPT_NAME)\n",
    "\n",
    "    print('[Done.]')\n",
    "    return ofname\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/feng/Templates/listImages.html\r\n"
     ]
    }
   ],
   "source": [
    "! ls /home/feng/Templates/listImages.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "??json.dumps({},sort_keys=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
