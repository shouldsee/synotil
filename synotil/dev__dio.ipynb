{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.000123\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "startTime = datetime.now()\n",
    "\n",
    "#do something\n",
    "\n",
    "#Python 2: \n",
    "print datetime.now() - startTime \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dio.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dio.py\n",
    "\n",
    "#### Data I/O\n",
    "import CountMatrix as scount\n",
    "import pyBigWig as pybw\n",
    "# import pymisca.util as pyutil\n",
    "import pymisca.shell as pysh\n",
    "import pymisca.ext as pyext\n",
    "import pymisca.util as pyutil\n",
    "pd = pyutil.pd; np = pyutil.np; \n",
    "# import synotil.dio as sdio\n",
    "\n",
    "\n",
    "bedHeader = '''\n",
    "0:chrom\n",
    "1:start\n",
    "2:end\n",
    "3:acc\n",
    "4:score\n",
    "5:strand\n",
    "6:FC\n",
    "7:neglogPval\n",
    "8:neglogQval\n",
    "9:summit\n",
    "10:img\n",
    "'''.strip().splitlines()\n",
    "bedHeader = [x.split(':')[1] for x in bedHeader] \n",
    "\n",
    "\n",
    "def readChipPeak(fname,**kwargs):\n",
    "    '''CountMatrix.geneList is buggy at the moment hence needed to be converted back into \n",
    "    CountMatrix.countMatrix\n",
    "'''\n",
    "    df = scount.countMatrix.from_DataFrame(fname=fname,ext='tsv',index_col='geneAcc',addFname=0,**kwargs)\n",
    "    df= df.toGeneList()\n",
    "    return df\n",
    "\n",
    "import StringIO\n",
    "def guessBedHeader(fname,silent=True,ext='tsv',guess_index=0, \n",
    "                   prefix = '', **kwargs):\n",
    "    cmd = 'head -n5 %s'%fname\n",
    "    buf = StringIO.StringIO(pyutil.shellexec(cmd,silent=silent))\n",
    "    df = pyutil.readData(buf,ext=ext,header=None,guess_index=guess_index,**kwargs)\n",
    "    if len(df.columns) >  len(bedHeader):\n",
    "        header = bedHeader + list(df.columns)[len(bedHeader):]\n",
    "    else:\n",
    "        header = bedHeader[:len(df.columns)]\n",
    "    if prefix: \n",
    "        header = ['%s_%s'%(prefix,x) for x in header]\n",
    "    return map(str,header)\n",
    "\n",
    "def header_closest(peak1,peak2):\n",
    "    '''Get the header for bedtools intersect -wo /closest -d\n",
    "'''\n",
    "    header = sum( [guessBedHeader(x,prefix=k) for k,x in \n",
    "                   [('',peak1),\n",
    "                    ('feat',peak2)]\n",
    "                  ],\n",
    "                 [])\n",
    "    header +=['distance']\n",
    "    return header\n",
    "\n",
    "def extract_peak(fname,ext='tsv',header=None,guess_index=0,**kwargs):\n",
    "    df = pyutil.readData(fname,ext=ext,header=header,guess_index=guess_index,**kwargs)\n",
    "    df.columns = bedHeader[:len(df.columns)] + list(df.columns)[len(bedHeader):]\n",
    "    return df\n",
    "    \n",
    "\n",
    "\n",
    "def npk_expandSummit(fname = None,df=None,radius=200,clip = 1,center_summit= 0):\n",
    "    '''\n",
    "    Expand the summit regions of a .narrowPeak dataFrame\n",
    "'''\n",
    "    if df is None:\n",
    "        df = extract_peak(fname)\n",
    "        \n",
    "    if 'abs_summit' not in df.columns:\n",
    "#         if 'summit' not in df.columns and :\n",
    "        if center_summit:\n",
    "            df['summit'] = (df.start + df.end )// 2\n",
    "            \n",
    "        assert 'summit' in df.columns\n",
    "        if (df.summit >= df.start).all():\n",
    "            pass\n",
    "        else:\n",
    "            df['summit'] = df.start + df.summit\n",
    "        df.rename(columns={'summit':'abs_summit'}, inplace=True)\n",
    "        df.abs_summit = df.abs_summit.astype('int')\n",
    "#       df = df.rename(columns={'summit':'abs_summit'}, )\n",
    "        \n",
    "#     st = df.strand\n",
    "    df.start = (df.abs_summit - radius)\n",
    "    df.end  = df.abs_summit + radius \n",
    "#     df.drop('abs_summit',1,inplace=True)\n",
    "    if clip:\n",
    "        df.start = df.start.clip_lower(0)\n",
    "        \n",
    "    if fname is not None:\n",
    "        base = pyutil.basename(fname)\n",
    "        ofname = '%s_radius=%d.tsv' % (base,radius)\n",
    "        df.to_csv(ofname,sep='\\t',index=None,header=None)\n",
    "        return ofname\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "def msg_bwFile(fname):\n",
    "#     import pyBigWig as pybw\n",
    "    f = pybw.open(fname)\n",
    "    print f.chroms()\n",
    "    f.close()    \n",
    "    return fname\n",
    "#     df.columns = [pyutil.basename(fname)]\n",
    "#     chipPeak = df\n",
    "#     return df\n",
    "\n",
    "def parseBedmap(fname = None, df = None, ):\n",
    "    ''' Parse the output of bedMap\n",
    "'''\n",
    "    if df is None:\n",
    "        df = pyutil.readData(fname,header = None,ext='tsv',guess_index=False)\n",
    "\n",
    "    df = df.dropna()\n",
    "    \n",
    "    df.columns = bedHeader + ['hit']\n",
    "\n",
    "    res = pyutil.explode(df,'hit','acc',';')\n",
    "    res = res.merge(df.drop('hit',1),on='acc')\n",
    "    return res\n",
    "\n",
    "def extract_closest(fname = None, df = None, ):\n",
    "    ''' Parse the output of 'bedtools closest'\n",
    "'''\n",
    "    if df is None:\n",
    "        df = pyutil.readData(fname,header = None,ext='tsv',guess_index=False)\n",
    "#     df = df.dropna()    \n",
    "\n",
    "    header = bedHeader + pyutil.paste0([['feature_'], bedHeader]).tolist()\n",
    "    df = df.iloc[:,:18]\n",
    "    df.columns = header[:17] + ['distance']\n",
    "    df['hit'] = df['feature_acc']\n",
    "    return df\n",
    "\n",
    "parseBedClosest = extract_closest \n",
    "\n",
    "def bed_randomise(infile,GSIZE = None,silent=1):\n",
    "    '''Create a randomly distributed bed file\n",
    "'''\n",
    "    ofile = pyutil.basename(infile) + '_type=random.bed'\n",
    "    assert GSIZE is not None\n",
    "    LC = pyutil.lineCount(infile)\n",
    "    cmd = \"bedtools random -g {GSIZE} -l 2 -n {LC}  | tee {ofile}\".format(**locals())\n",
    "    pyutil.shellexec(cmd,silent=silent)\n",
    "    return ofile\n",
    "\n",
    "def bed__summit(peakFile,GSIZE=None,silent=1,opt='-s -l -0.5 -r -0.5 -pct',\n",
    "               inplace=True):\n",
    "    if GSIZE is None:\n",
    "        GSIZE=pyutil.os.environ.get('GSIZE',None)\n",
    "    assert GSIZE is not None\n",
    "    ofname = '%s.summit' % peakFile\n",
    "    if not inplace:\n",
    "        ofname = pyutil.os.path.basename(ofname)\n",
    "    cmd = 'cat {peakFile} \\\n",
    "    | bedtools slop -g {GSIZE} {opt} -i - \\\n",
    "    > {ofname}'.format(**locals())\n",
    "    pyutil.shellexec(cmd,silent=silent)\n",
    "    return ofname\n",
    "def bed__leftSummit(peakFile,opt='-s -pct -l 0. -r -1.0',**kwargs):\n",
    "    return bed__summit(peakFile,opt=opt,**kwargs)\n",
    "def bed__rightSummit(peakFile,opt='-s -pct -r 0. -l -1.0',**kwargs):\n",
    "    return bed__summit(peakFile,opt=opt,**kwargs)\n",
    "# bed__leftSummit = pyutil.functools.partial(bed__summit,\n",
    "#                                            opt='-s -pct -l 0. -r -1.0')\n",
    "\n",
    "def bed__addCol__interval(bed,\n",
    "                          name='interval',\n",
    "                          expr = '@pyutil.paste0([chrom, \":\",start,[\"-\"],end]).tolist()',\n",
    "                         ):\n",
    "    pyutil.df__addCol(bed, name, expr )\n",
    "    return bed\n",
    "\n",
    "import StringIO\n",
    "import numpy as np\n",
    "\n",
    "def bed__guessWidth(bedFile,silent=1,head=100): \n",
    "    res = pyutil.shellexec('head -n{head} {bedFile}'.format(**locals()),\n",
    "                           silent=silent)\n",
    "    dfc = extract_peak(StringIO.StringIO(res))\n",
    "    span = (dfc.end - dfc.start).values.ravel()\n",
    "    M = np.median(span)\n",
    "    if span.std() / len(span)**0.5 > 0.1 * M:\n",
    "        pyutil.sys.stderr.write ('[WARN]:estimation may be unstable\\n')\n",
    "    return int(M) \n",
    "\n",
    "def summitDist(peak1,peak2,\n",
    "               CUTOFF = 400,\n",
    "            silent = 1,\n",
    "              GSIZE = None,\n",
    "               as_fname=0,\n",
    "              **kwargs):\n",
    "    '''Find nearby summits within a distance cutoff\n",
    "'''\n",
    "    if GSIZE is None:\n",
    "        GSIZE=pyutil.os.environ.get('GSIZE',None)\n",
    "    assert GSIZE is not None\n",
    "    RANGE  = CUTOFF//2 - 1\n",
    "    infiles = [peak1,peak2]\n",
    "#     def file_ncol(fname):\n",
    "#         cmd = 'wc -l %s'%(fname)\n",
    "#         res = pyutil.shellexec(cmd,silent=silent)\n",
    "#         ncol = res[0].strip().split('\\t')\n",
    "#     incols = \n",
    "    incols = map(pyutil.file_ncol,infiles)\n",
    "\n",
    "    ### padding/inflate the summit to have radius\n",
    "    lst = []\n",
    "    for infile in infiles:\n",
    "        \n",
    "        ofile = \"{infile}.{RANGE}\".format(**locals()).split('/')[-1]\n",
    "        lst += [ofile]\n",
    "\n",
    "        cmd = \"bedtools slop -g {GSIZE} -b {RANGE} -i {infile} \\\n",
    "          | tee {ofile}\".format(**locals())\n",
    "        _ = pyutil.shellexec(cmd,silent=silent)\n",
    "\n",
    "    slop1,slop2 = lst\n",
    "    FOUT = 'infiles:'+ \":\".join(map(pyutil.basename,infiles)) \\\n",
    "        + \"__cutoff:{}.tsv\".format(CUTOFF)\n",
    "\n",
    "    # ### bed format 1=chrom, 2=start, 3=end\n",
    "    # cols = ','.join(map(str,[2,3,] + [x + incols[0] for x in [2,3]]))\n",
    "    # cmd = \"bedtools closest -a {slop1} -b {slop2} \\\n",
    "    #   | bedtools overlap -cols {cols} \\\n",
    "    #   | tee {FOUT}\".format(**locals())\n",
    "\n",
    "    cmd = \"bedtools intersect -wo -a {slop1} -b {slop2} \\\n",
    "      | tee {FOUT}\".format(**locals())\n",
    "\n",
    "\n",
    "    buf = pyutil.shellexec(cmd,silent=silent)\n",
    "\n",
    "    ### [TBC]Memory-intensive, Replace with awk mutation in the future\n",
    "    columns = header_closest(peak1,peak2)\n",
    "    \n",
    "    df = pyutil.readData( StringIO.StringIO(buf), header = None, ext='tsv',guess_index=False, \n",
    "                         columns = columns)\n",
    "    df.distance = CUTOFF - df.distance\n",
    "    df.to_csv(FOUT, sep='\\t',index = False)\n",
    "    if as_fname:\n",
    "        return FOUT\n",
    "    else:\n",
    "        return df\n",
    "    \n",
    "def bed__merge(bedFile,silent=1,opt='-c 4 -o first'):\n",
    "    bname = pyutil.os.path.basename(bedFile)\n",
    "    path = pyutil.os.path.dirname(bedFile)\n",
    "    ofname = pyutil.os.path.join(path,'merged__%s'%bname)\n",
    "    cmd = 'bedtools sort -i {bedFile} |  bedtools merge -i - {opt} > {ofname}'.format(**locals())\n",
    "    pyutil.shellexec(cmd,silent=silent)\n",
    "    return ofname\n",
    "\n",
    "def bed__makewindows(bedFile,windowSize=100,stepSize=None,silent =1 ):\n",
    "    if stepSize is None:\n",
    "# windowSize=100\n",
    "        stepSize = windowSize//2\n",
    "# bedFile = 'per-score-GT-0dot6_188C_RESEQ-combined.bed'\n",
    "#         bedbase = pyutil.bname(bedbane)\n",
    "    ofname = '{bedFile}.w{windowSize}s{stepSize}'.format(**locals())\n",
    "    cmd = \"bedtools makewindows -i srcwinnum -w {windowSize} -s {stepSize} -b {bedFile} > {ofname} \".format(**locals())\n",
    "    pyutil.shellexec(cmd,silent = silent)\n",
    "    return ofname\n",
    "\n",
    "\n",
    "    \n",
    "def extract_bigwig_worker(lines, bwFile = None,stepSize = 1, stranded= 1, bw = None):\n",
    "    ''' Helper mapper for querying BigWig\n",
    "'''\n",
    "    bw = pybw.open(bwFile)\n",
    "    chromL = bw.chroms()\n",
    "    \n",
    "    lines = [x for x in lines if x]\n",
    "    nField = lines[0].strip().split('\\t').__len__() \n",
    "    res = []\n",
    "    for line in lines:\n",
    "#     def parse(line, nField = nField):\n",
    "        if line is None:\n",
    "            return None\n",
    "        cols = line.strip().split('\\t')\n",
    "        if nField >= 6:\n",
    "            chrom, start,end, (id, score, strand) = cols[0],int(cols[1]),int(cols[2]),cols[3:6]\n",
    "        else:\n",
    "            strand = '+'\n",
    "            if nField is 5:\n",
    "                chrom, start,end, id,_ = cols[0],int(cols[1]),int(cols[2]),cols[3],cols[4]\n",
    "                \n",
    "#                 assert 0, 'operation not defined when bedFile has 5 fields:\\n%s'%lines[0]\n",
    "            elif nField is 4:\n",
    "                chrom, start,end, id = cols[0],int(cols[1]),int(cols[2]),cols[3]\n",
    "            else:\n",
    "                chrom, start,end = cols[0],int(cols[1]),int(cols[2])\n",
    "                id = 'NoID'\n",
    "                \n",
    "        if chrom not in bw.chroms():\n",
    "            o = None\n",
    "        else:\n",
    "            start = max(0,start)\n",
    "            end = min(chromL[chrom],end)\n",
    "            sec = bw.values(chrom, start, end, numpy=0)\n",
    "            if strand is not '-' or not stranded:\n",
    "                vals = sec[::stepSize]\n",
    "            else:\n",
    "                vals = sec[::-stepSize]\n",
    "                \n",
    "            o = vals\n",
    "#         return (id,o)\n",
    "        res+=[(id,o)]\n",
    "#     res = map( parse, lines) \n",
    "    bw.close()\n",
    "    return res\n",
    "\n",
    "def extract_bigwig(bwFile,bedFile,\n",
    "                   stepSize=1,\n",
    "                  mapChunk = None, \n",
    "#                   span = None\n",
    "                   shift=1,\n",
    "#                   outIndex = None,\n",
    "                   stranded=1,\n",
    "                 ):\n",
    "    ''' Extracting a signal matrix for each bed region\n",
    "'''\n",
    "#     assert NCORE == 1,'Multi-thread is slower here..., so dont! '\n",
    "\n",
    "#     assert stepSize == 1,'Not implemented'        \n",
    "    with pybw.open(bwFile) as bw:\n",
    "        it = open(bedFile)\n",
    "        worker = pyutil.functools.partial(extract_bigwig_worker,\n",
    "                                          bwFile =bwFile,\n",
    "                                         stepSize=stepSize,\n",
    "                                         stranded=stranded,)\n",
    "        if  1 == 1:\n",
    "            res = map(worker,[it])\n",
    "\n",
    "            \n",
    "        res = sum(res,[])\n",
    "#             pass \n",
    "        ids, out  = zip(*res)\n",
    "\n",
    "    #### Replacing \"None\" and incomplete intervals\n",
    "    ref = next((item for item in out if item is not None),None)\n",
    "    assert ref is not None,'Cannot find an reference shape, likely wrong chromosomes.\\n\\\n",
    "    bigwigFile:\"%s\" '%bwFile\n",
    "#     L = len(ref)\n",
    "#     L = len(res) if span is None else span //stepSize        \n",
    "    L = max(map(len,out))\n",
    "    lst = []\n",
    "    print '[L]=',L\n",
    "    for x in out:\n",
    "        if x is None:\n",
    "            y = [0.]*L\n",
    "        else:\n",
    "            Lx = len(x)\n",
    "            y = x + [0.] * (L - Lx)            \n",
    "        lst += [y]\n",
    "#         out = [[0.]*L if x is None else x for x in out]\n",
    "    out = np.array(lst)\n",
    "    out = np.nan_to_num(out)\n",
    "    \n",
    "#     MLEN = np.mean([len(x) for x in out]) \n",
    "    MLEN='not set'\n",
    "    assert out.dtype!='O','''Unable to shape the matrix properly: \n",
    "    %s, %s '''% (MLEN, [(type(x),x) for x in out if len(x)< MLEN] )\n",
    "    out = pd.DataFrame(out).set_index([list(ids)])\n",
    "        \n",
    "    cols = stepSize * ( np.arange(0, out.shape[-1], )  ) \n",
    "    if shift:\n",
    "        mid = ( L * stepSize )//2\n",
    "        cols += -mid\n",
    "    out.columns = cols\n",
    "    \n",
    "#     out.columns = (stepSize * np.arange(0, out.shape[-1], ))\n",
    "            # Do something with the values...\n",
    "    \n",
    "\n",
    "#     out = ctMat.countMatrix.from_DataFrame(df=out)\n",
    "#     out.fname = bwFile\n",
    "    out.param = pyutil.util_obj()\n",
    "    out.param['bwFile'] = bwFile\n",
    "    out.param['bedFile'] = bedFile\n",
    "    return out\n",
    "\n",
    "def extract_bigwig_multiple(bedFile= None,peakFile = None,\n",
    "                            bwFiles = None,fnames=None,\n",
    "                            radius = None,\n",
    "                            callback= lambda x:pd.concat(x,axis=1),\n",
    "                            NCORE=1,\n",
    "                            stepSize=20,\n",
    "                            outIndex=pyutil.basename,\n",
    "                            center_summit = 0,**kwargs):\n",
    "    '''extract_bigwig() for multiple bwFiles\n",
    "'''\n",
    "    #########################\n",
    "    #### Legacy support #####\n",
    "    if bedFile is None:\n",
    "        assert peakFile is not None\n",
    "        bedFile =  peakFile\n",
    "    else:\n",
    "        assert peakFile is None\n",
    "        \n",
    "    if bwFiles is None:\n",
    "        assert fnames is not None\n",
    "        bwFiles = fnames\n",
    "    #### Legacy support ####\n",
    "    #########################\n",
    "        \n",
    "    if radius is not None:\n",
    "        bedFile = npk_expandSummit(fname=bedFile,\n",
    "                                   radius=radius,center_summit =center_summit)\n",
    "    print '[L]',pyutil.lineCount(bedFile)\n",
    "\n",
    "    #### Compute Matrix\n",
    "\n",
    "    worker = pyutil.functools.partial(\n",
    "        extract_bigwig,\n",
    "#         bwFile=fname,\n",
    "        stepSize=stepSize,\n",
    "        bedFile=bedFile,\n",
    "#         outIndex=outIndex,\n",
    "#         NCORE=1,\n",
    "        **kwargs) \n",
    "    bws = pyutil.mp_map(worker, bwFiles, n_cpu = NCORE)\n",
    "#     bws = [ for fname in fnames]\n",
    "\n",
    "    for i,(bwFile,out) in enumerate(zip(bwFiles,bws)):\n",
    "        replaceCol  = None\n",
    "        if outIndex is None:\n",
    "            replaceCol = None\n",
    "        else:\n",
    "            if callable(outIndex):\n",
    "                replaceCol = outIndex(bwFile)\n",
    "            else:\n",
    "                replaceCol = outIndex[i]\n",
    "        if replaceCol is not None:\n",
    "            #### replace outer index,replace with rename()\n",
    "            #### [TEMP]\n",
    "            tmp = out.T\n",
    "            tmp['ind'] = replaceCol\n",
    "            tmp.set_index( 'ind', append=1,inplace=True)\n",
    "            tmp = tmp.reorder_levels(['ind',None])        \n",
    "            tmp.index.names = ['bwFile','pos']\n",
    "            out = tmp.T\n",
    "        out.index.name = 'acc'\n",
    "        bws[i] = out\n",
    "        \n",
    "    dfc =  bws\n",
    "    \n",
    "    \n",
    "#     dfc=pd.concat(bws,)\n",
    "#     dfc = pd.concat(bws,axis=1) #### abosorbed into callback()\n",
    "    \n",
    "    if callback is not None:\n",
    "        dfc = callback(dfc)\n",
    "    return dfc\n",
    "\n",
    "# def bigwig_average_signal(dfc,):\n",
    "#     L = len(dfc)\n",
    "#     dfc = pyutil.colGroupMean(dfc)\n",
    "#     dfc = scount.countMatrix(dfc.T)\n",
    "#     dfc.param['n_peak'] = L\n",
    "#     return dfc\n",
    "\n",
    "\n",
    "sdio = pyutil.sys.modules[__name__]\n",
    "def job__nearAUG(peakFile = None,featFile =None,\n",
    "                 peakSummit= None, featSummit = None, \n",
    "                 CUTOFF=6000,peakWid=None,GSIZE=None):\n",
    "    JOB='nearAUG'\n",
    "    if peakSummit is None:\n",
    "        assert peakFile is not None\n",
    "        peakSummit = sdio.bed__summit(peakFile,GSIZE=GSIZE,inplace=0)\n",
    "    if featSummit is None:\n",
    "        assert featFile is not None\n",
    "        featSummit = sdio.bed__leftSummit(featFile,GSIZE=GSIZE,inplace=0)\n",
    "\n",
    "    if peakWid is None:\n",
    "        peakWid= sdio.bed__guessWidth(peakFile)//2 \n",
    "        \n",
    "    res = sdio.summitDist(\n",
    "        peakSummit,\n",
    "        featSummit,\n",
    "        CUTOFF= 6000 - peakWid,\n",
    "        GSIZE=GSIZE\n",
    "    )\n",
    "    \n",
    "    out = sdio.extract_peak(peakSummit).merge(\n",
    "        res[['acc','feat_acc','distance']],\n",
    "#         res.drop(columns=['chrom','start','end']),\n",
    "        how='right',\n",
    "        left_on='acc',right_on='acc').query(\"distance < %d\" % CUTOFF)\n",
    "\n",
    "    featSummitBase = pyutil.basename(featSummit).replace('_','-')\n",
    "    peakSummitBase = pyutil.basename(peakSummit).replace('_','-')\n",
    "    ofname = '\\\n",
    "job_{JOB}__\\\n",
    "peak_{peakSummitBase}__\\\n",
    "cutoff_{CUTOFF}__\\\n",
    "feat_{featSummitBase}.tsv'.format(**locals())\n",
    "#     pyutil.to_tsv(out,ofname)   \n",
    "    out.to_csv(ofname,sep='\\t')\n",
    "#     pyutil.to_tsv(out,ofname,header=True,index=1)   \n",
    "    return ofname\n",
    "\n",
    "\n",
    "###### [TBC] belongs to LookupTable in the future\n",
    "def peak2gene(pgf,query,**kwargs):\n",
    "    res = pgf.merge(query,left_on='acc',right_index=True,**kwargs)\n",
    "    return res\n",
    "def gene2peak(pgf,query,**kwargs):\n",
    "    res = pgf.merge(query,left_on='feat_acc',right_index=True,**kwargs)\n",
    "    return res\n",
    "\n",
    "def df__sanitise__Ath(df):\n",
    "    df.index = df.index.str.extract(\".*(AT\\dG\\d{5}).*\",expand=False)\n",
    "    return df\n",
    "####\n",
    "\n",
    "def getChrom(fname):\n",
    "    f = pybw.open(fname)\n",
    "    chrom = f.chroms()\n",
    "    f.close()\n",
    "    return chrom\n",
    "\n",
    "def check__bigwig__chrom(fnames,chrom=None):\n",
    "    \n",
    "    chroms = map(getChrom, fnames)\n",
    "    if chrom is None:\n",
    "        chrom = chroms[0]\n",
    "    good = map(lambda x: x==chrom, chroms)\n",
    "    return pd.Series(good,index=fnames,name='check__bigwig__chrom')    \n",
    "\n",
    "\n",
    "def assign__filename(lst,runID='000R',ext='excel.count',DIR='.',init=1):\n",
    "    '''\n",
    "    Internal routine for creating data structure\n",
    "'''\n",
    "    out = []\n",
    "    DIR = DIR.rstrip('/')\n",
    "    fmt = '{DIR}/{runID}/S{sampleID_int}/{ele}_S{sampleID_int}.{ext}'\n",
    "    for i,ele in enumerate(lst):\n",
    "        sampleID_int = i + init\n",
    "        ele = ele.replace('_','-')\n",
    "        res = fmt.format(**locals())\n",
    "        out +=[res]\n",
    "    return out\n",
    "\n",
    "def df__deposit(dfc,runID='000R',ext='excel.count',DIR='.',init=1,silent=0,\n",
    "               sep='\\t',header=1):\n",
    "#     dfc = rnaseq.copy()\n",
    "    dfc = dfc.copy()\n",
    "    fnames = assign__filename(dfc.columns,\n",
    "                              runID=runID,ext=ext,DIR=DIR,init=init)\n",
    "\n",
    "    dfc.columns = fnames\n",
    "    dfc.columns.name = 'fname'\n",
    "    gp = dfc.reset_index().melt(value_name='TPM',id_vars=['gene_id']).groupby('fname')\n",
    "    for fname,df in gp:\n",
    "        odf = df.drop(columns='fname').set_index('gene_id')\n",
    "        pyutil.shellexec('mkdir -p `dirname {fname}`'.format(**locals()),silent=silent,)\n",
    "        odf.to_csv(fname,sep=sep,header=header)\n",
    "    return dfc.columns\n",
    "\n",
    "\n",
    "idKeys = ['runID','sampleID','read']\n",
    "\n",
    "def rawFile__validateChunk(dfc,):\n",
    "    '''Validate the df_raw\n",
    "'''\n",
    "    idKeys = ['runID',\n",
    "              'sampleID',\n",
    "              'read']\n",
    "    dfc = dfc.sort_values(idKeys + ['chunk'])\n",
    "    gp  = dfc.groupby(idKeys)\n",
    "    for (key,df) in gp:\n",
    "        assert len(df) == 4,key\n",
    "    if not dfc['ext'].iloc[0].startswith('.'):\n",
    "        dfc['ext'] = dfc['ext'].map(lambda x:'.%s'%x)\n",
    "    dfc['fnameCombined'] = pyutil.df__paste0(dfc,\n",
    "                                              idKeys + ['ext'],\n",
    "#                                               headerFmt='_',\n",
    "                                              sep='_',\n",
    "                                              ).tolist()\n",
    "    dfc['fnameCombinedSize'] = 0\n",
    "    return dfc\n",
    "def rawFile__combineChunk(dfc,silent=0):\n",
    "    ''' combine chunkedFiles in df_raw\n",
    "    according to \"fname\" and \"fnameCombined\"\n",
    "'''\n",
    "    dfc = dfc.copy()\n",
    "#     dfc = dfc[idKeys + ['fnameCombined','fnameCombinedSize']].drop_duplicates()\n",
    "    \n",
    "    fnameFlat = ' \\\\\\n'.join(dfc.fname)\n",
    "    ofnames = dfc.fnameCombined.unique()\n",
    "    assert len(ofnames)==1,\\\n",
    "    'contains mulitple fnameCombined!:%s'%ofnames\n",
    "    ofname = ofnames[0]\n",
    "    cmd = 'cat {fnameFlat} > {ofname}'.format(**locals())\n",
    "    res= pyutil.shellexec(cmd,silent=silent)\n",
    "    dfc['fnameCombinedSize'] = pyutil.os.path.getsize(ofname)\n",
    "    dfc = dfc[idKeys + ['fnameCombined','fnameCombinedSize']\n",
    "             ].drop_duplicates()\n",
    "    return dfc\n",
    "\n",
    "def worker__rawFile__combineChunk((key,dfc)):\n",
    "    '''not fast, dont use'''\n",
    "    return rawFile__combineChunk(dfc,silent=0)\n",
    "\n",
    "def bam__getHeader(fname,grepKey='SQ',silent=1,head=100):\n",
    "    cmd = u'samtools view -H %s' % fname \n",
    "    if grepKey is not None:\n",
    "        cmd = u'{cmd} | grep {grepKey}'.format(**locals())\n",
    "    if head is not None:\n",
    "        cmd = u'{cmd} | head -n{head}'.format(**locals())\n",
    "    res = pyutil.shellexec(cmd,silent=silent)\n",
    "    return res\n",
    "\n",
    "def listByChip(dfc):\n",
    "    '''Transform a bwTracks object\n",
    "'''\n",
    "    dfcc = pd.DataFrame(iter(dfc.groupby(axis=1,level='bwFile')),\n",
    "                        columns=['bwFile', 'tracks'],\n",
    "                       )\n",
    "    for df in dfcc['tracks']:\n",
    "        key = df.columns.levels[0][0]\n",
    "        df.columns = df.columns.droplevel(0)\n",
    "        df.index =  pd.MultiIndex.from_arrays([\n",
    "            [key]*len(df),\n",
    "            df.index],\n",
    "            names=['bwFile','acc'],\n",
    "        )\n",
    "#         df['bwFile'] ='test'\n",
    "#         df.set_index('bwFile', append=True,inplace=True)\n",
    "#     dfcc['tracks']  =dfcc['tracks'].map(lambda x:\n",
    "#                                         x.columns=x.columns.drop_level())\n",
    "    return dfcc\n",
    "\n",
    "def listByGene(dfc):\n",
    "    ''' Transform a bwTracks object\n",
    "'''\n",
    "    def pivotRecord(x):\n",
    "        res = x.melt().pivot_table(index='bwFile',\n",
    "                                         columns='pos',)\n",
    "        return res\n",
    "    dfcc = dfc.groupby(axis=0,level=0).apply(pivotRecord)\n",
    "#     dfcc.index = dfcc.index.rename(level=0,names='acc')\n",
    "    return dfcc\n",
    "\n",
    "def listByGene(dfc,concat=1):\n",
    "#     idvars = dfc.columns.names\n",
    "    if not concat:\n",
    "        raise Exception('Not implemented')\n",
    "        \n",
    "    dfcc = dfc.T.reset_index().melt(id_vars = ['bwFile','pos']).pivot_table(\n",
    "        index=['acc','pos'],\n",
    "        columns='bwFile')\n",
    "    return dfcc\n",
    "\n",
    "# def listByPos(dfc):\n",
    "#     ''' Transform a bwTracks object\n",
    "# '''\n",
    "#     def pivotRecord(x):\n",
    "#         res = x.melt().pivot_table(index='bwFile',\n",
    "#                                    columns='pos',\n",
    "#                                          )\n",
    "#         return res\n",
    "#     dfcc = dfc.groupby(axis=0,level=0).apply(pivotRecord).T\n",
    "#     return dfcc\n",
    "\n",
    "def file__concat(bedFiles,silent=1,\n",
    "                ofname = 'concated_file',\n",
    "                ext = None\n",
    "#                 opt='-c 4 -o first'\n",
    "               ):\n",
    "    '''Concatentat a list of files\n",
    "'''\n",
    "    if ext is None:\n",
    "        sp = bedFiles[0].rsplit('.',1)\n",
    "        if len(sp) == 2:\n",
    "            ext = sp[-1]\n",
    "        else:\n",
    "            ext = None\n",
    "    if ext is not None:\n",
    "        ofname = '.'.join([ofname,ext])\n",
    "    flatName = ' '.join(bedFiles)\n",
    "    cmd = 'cat {flatName} >{ofname}'.format(**locals())\n",
    "#     bname = pyutil.os.path.basename(bedFile)\n",
    "#     path = pyutil.os.path.dirname(bedFile)\n",
    "#     ofname = pyutil.os.path.join(path,'merged__%s'%bname)\n",
    "#     cmd = 'bedtools sort -i {bedFile} |  bedtools merge -i - {opt} > {ofname}'.format(**locals())\n",
    "    pyutil.shellexec(cmd,silent=silent)\n",
    "    return ofname\n",
    "\n",
    "def bed__totalLength(bedFile,silent=1):\n",
    "    '''Source: https://www.biostars.org/p/68283/#68292\n",
    "'''\n",
    "    cmd = \"cat %s | awk -F'\\t' 'BEGIN{SUM=0}{ SUM+=$3-$2 }END{print SUM}'\" % bedFile\n",
    "    res = pyutil.shellexec(cmd,silent=silent)\n",
    "    res = int(res.strip())\n",
    "    return res\n",
    "\n",
    "def bed__embed(outerBed,innerBed,debug=0,ofname = None,mergeAcc=1):\n",
    "    if not isinstance(outerBed,pd.DataFrame):\n",
    "        outerBed=  sdio.extract_peak(outerBed)\n",
    "    if not isinstance(innerBed,pd.DataFrame):\n",
    "        innerBed=  sdio.extract_peak(innerBed)\n",
    "#             lc[key] = sdio.extract_peak(val)\n",
    "    assert 'acc' in outerBed,'Reference bed file must be named'\n",
    "    if 'acc' in innerBed:\n",
    "        if mergeAcc:\n",
    "            innerBed['acc'] = pyutil.df__paste0(innerBed,\n",
    "                                                ['chrom','acc'],sep='_').tolist()\n",
    "    for df in [outerBed,innerBed]:\n",
    "        if 'strand' not in df.columns:\n",
    "            df['strand'] = '+'\n",
    "        df.strand.fillna('+')\n",
    "        df['strandVal'] = df.strand.isin(['+','*'])\n",
    "    res = innerBed.merge(outerBed,how='left',\n",
    "                         left_on='chrom',\n",
    "                         right_on='acc',\n",
    "#                          prefixes=['t','1'],\n",
    "                        suffixes=['Inner','Outer'],)\n",
    "    res['strandValFinal'] = ~( (res['strandValOuter']) ^ (res['strandValInner']) )\n",
    "    isNeg = res.strandValFinal==0\n",
    "    (res.loc[ isNeg,'startInner'], res.loc[isNeg,'endInner']) \\\n",
    "        = (-res.loc[isNeg,'endInner'],-res.loc[isNeg,'startInner'])\n",
    "    res['valid'] = res.eval('startInner<=endInner')\n",
    "    assert res.valid.all()\n",
    "#     print 'acc' in res.columns\n",
    "    if debug ==2:\n",
    "        return res\n",
    "\n",
    "    if debug ==1:\n",
    "        return res[['valid',\n",
    "                    'chromInner',\n",
    "                    'startInner',\n",
    "                    'endInner',\n",
    "                    'strandValFinal',\n",
    "                    'strandValInner',\n",
    "                    'strandValOuter']]\n",
    "    res['shift'] = 0\n",
    "    isOuterNeg = res.strandValOuter == 0\n",
    "    res.loc[isOuterNeg, 'shift'] = res.loc[isOuterNeg,'endOuter']\n",
    "    res.loc[~isOuterNeg, 'shift'] = res.loc[~isOuterNeg,'startOuter']\n",
    "#     res.shift = r\n",
    "    res['start'] = res['shift'] + res['startInner']\n",
    "    res['end'] = res['shift'] + res['endInner']\n",
    "    res['chrom'] = res['chromOuter']\n",
    "    res['acc'] = res['accInner']\n",
    "    resDF = res[['chrom',\n",
    "                    'start',\n",
    "                    'end',\n",
    "                     'acc'\n",
    "#                      'accInner'\n",
    "                   ]]\n",
    "    if ofname is not None:\n",
    "        try:\n",
    "            pyutil.to_tsv(resDF,ofname,)\n",
    "            return ofname\n",
    "        except Exception as e:\n",
    "            print e\n",
    "    return resDF\n",
    "\n",
    "def tsv__getColumns(fname,ext='tsv'):\n",
    "#     pyutil.readData()\n",
    "    res = file__header(fname,silent=silent)\n",
    "    df = pyutil.readData(res,ext=ext)\n",
    "    return df.columns.tolist()\n",
    "\n",
    "def file__header(fname,head = 10,silent=1):\n",
    "    res = pyutil.shellexec('head -n{head} {fname}'.format(**locals()),\n",
    "                          silent=silent)\n",
    "    res = pyutil.StringIO.StringIO(res)\n",
    "    return res\n",
    "\n",
    "def count__getGeneHeader(fname, \n",
    "                         ext='tsv',\n",
    "                         pipeline=None, silent=1, **kwargs):\n",
    "    ext = 'tsv' ### hard set\n",
    "    res = file__header(fname,silent=silent)\n",
    "    df = pyutil.readData(res,ext=ext,guess_index=0)\n",
    "    return df.gene_id.tolist()\n",
    "\n",
    "def count__getGeneHeader__dict(data,head=10):\n",
    "#     data['ext'] = data['pipeline']\n",
    "    res =  count__getGeneHeader(head=head,**data)\n",
    "    return res\n",
    "    \n",
    "\n",
    "def wig2bigwig(fname,chromSizes = 'chrom.sizes',silent=1):\n",
    "    ofbase = pyutil.getBname(fname) \n",
    "    ofname = '%s.bw' % ofbase\n",
    "    cmd = '''wigToBigWig {fname} {chromSizes} {ofname}\n",
    "    '''.format(**locals())\n",
    "    res = pyutil.shellexec(cmd,silent=silent)\n",
    "    return ofname\n",
    "\n",
    "def clu2bed(segDF, ofname=None):\n",
    "    '''Must have columns: ('acc','pos','clu')\n",
    "    '''\n",
    "    segDF = segDF.reset_index()\n",
    "#     stdout,isFile = get__stdout(ofname)\n",
    "    stepSize = np.diff(segDF['pos'].values[:2], axis=0)[0]\n",
    "    vals = segDF[['clu','acc']].values    \n",
    "    isDiff = (vals[1:] != vals[:-1]).any(axis=1)\n",
    "    segDF['isDiff'] = np.concatenate([\n",
    "                [True],\n",
    "                isDiff],axis=0)\n",
    "    it = (pyutil.util_obj(**vars(x)) for x in segDF.itertuples())\n",
    "    peak = pyutil.collections.OrderedDict(\n",
    "        (\n",
    "        ('chrom',None),\n",
    "        ('start',None),\n",
    "        ('end',None),\n",
    "        ('acc',None),\n",
    "        )\n",
    "    )\n",
    "    peaks = []\n",
    "    def savePeakStart():\n",
    "        peak['chrom'] = rec.acc\n",
    "        peak['start'] = rec.pos\n",
    "        return\n",
    "    def savePeakEnd():\n",
    "#         kk = loc\n",
    "        peak['end'] = oldPos + stepSize\n",
    "        peak['acc'] ='summitPos%d'%( ( peak['start'] + peak['end']) //2)\n",
    "        assert peak['end'] > peak['start'], peak\n",
    "#         pyutil.ppJson(locals())\n",
    "        peaks.append(peak.copy())\n",
    "        \n",
    "#         line = u'\\t'.join(map(unicode,peak.values()))\n",
    "#         stdout.write(u'%s\\n'%line)\n",
    "\n",
    "#         print peak\n",
    "        return\n",
    "    def changed():\n",
    "        if idx != 0:\n",
    "            if oldClu == 1:\n",
    "                savePeakEnd()\n",
    "            if rec.clu == 1:\n",
    "                if (oldClu == 0) | (oldAcc != rec.acc):\n",
    "                    savePeakStart()\n",
    "        else:\n",
    "            if rec.clu == 1:\n",
    "                savePeakStart()\n",
    "        return\n",
    "    \n",
    "    #### Starting the loop\n",
    "    oldClu = 0 \n",
    "    for idx,rec in enumerate(it):\n",
    "        if (idx==0):\n",
    "            changed()\n",
    "        elif (rec.clu!=oldClu) or (rec.acc!=oldAcc):\n",
    "            changed()\n",
    "        oldClu = rec.clu    \n",
    "        oldPos = rec.pos\n",
    "        oldAcc = rec.acc\n",
    "    changed()\n",
    "    \n",
    "    resDF = pd.DataFrame(peaks)\n",
    "    \n",
    "    if ofname is not None:\n",
    "        try:\n",
    "            pyutil.to_tsv(resDF,ofname,)\n",
    "            return ofname\n",
    "        except Exception as e:\n",
    "            print e\n",
    "    return resDF\n",
    "\n",
    "\n",
    "# sizeDF.columns = ['chrom','length']\n",
    "def bed__checkValid(bed, GSIZE, force=0):\n",
    "    fname = None\n",
    "    if not isinstance(bed,pd.DataFrame):\n",
    "        fname = bed\n",
    "        bed = sdio.extract_peak(bed)\n",
    "    sizeDF = pyutil.readData(GSIZE,ext='tsv',header=None,guess_index=0)\n",
    "    sizeDF.columns = ['chrom','length']\n",
    "    bedDF =  sizeDF.merge(bed)\n",
    "    bedDF['valid'] = bedDF.eval('start > 0 and end <= length')\n",
    "    if not force:\n",
    "        assert bedDF.valid.all()\n",
    "    else:\n",
    "        resDF = bedDF.query('valid').drop(columns=['valid','length'])\n",
    "        if fname is not None:\n",
    "            ofname = '%s__valid.bed' % fname.rsplit('.',1)[0]\n",
    "            pyutil.to_tsv(resDF,ofname)\n",
    "            return ofname\n",
    "        else:\n",
    "            return resDF\n",
    "        \n",
    "        \n",
    "def sra__dump(uri,\n",
    "#               level=2,\n",
    "              silent=1,\n",
    "              baseFile=0):\n",
    "    uri = pyext.base__file(uri,baseFile=baseFile)\n",
    "#     head,tail = pyext.splitPath(uri)\n",
    "    CMD = 'fastq-dump --split-files {uri} 2>LOG'.format(**locals())\n",
    "    res = pysh.shellexec(CMD,silent=silent)\n",
    "    return res        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
