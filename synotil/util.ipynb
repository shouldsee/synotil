{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "####### Utilities for Visualising RNASeq. Author: Feng Geng (fg368@cam.ac.uk)\n",
    "#### Written for BrachyPhoton at SLCU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook util.ipynb to python\n",
      "[NbConvertApp] Writing 44968 bytes to util.py\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    !jupyter nbconvert --to python util.ipynb\n",
    "# !python compile_meta.ipynb && echo '[succ]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (util.py, line 1318)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/home/feng/.local/lib/python2.7/site-packages/pymisca/util.py\"\u001b[0;36m, line \u001b[0;32m1318\u001b[0m\n\u001b[0;31m    if avg == 'mean:\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os,re,sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymisca.vis_util as pyvis\n",
    "plt=pyvis.plt\n",
    "import pymisca.util as pyutil\n",
    "np = pyutil.np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "import sys\n",
    "sutil = modCurr = sys.modules[__name__]\n",
    "\n",
    "\n",
    "def to_tsv(df,fname,header= None,index=None, **kwargs):\n",
    "#     df =df.reset_index()[[0,1,2,'index',4,5,6]]\n",
    "    df.to_csv(fname,sep='\\t',header= header, index= index, **kwargs)\n",
    "    return fname\n",
    "\n",
    "\n",
    "\n",
    "def readLines(fname):\n",
    "    with open(fname,'r') as f:\n",
    "        lines = [l.rstrip('\\n') for l in f.readlines()]\n",
    "    return lines\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def discrete_cmap(N, base_cmap=None):\n",
    "    \"\"\"Create an N-bin discrete colormap from the specified input map\n",
    "    Source: https://gist.github.com/jakevdp/91077b0cae40f8f8244a\n",
    "    \"\"\"\n",
    "\n",
    "    # Note that if base_cmap is a string or None, you can simply do\n",
    "    #    return plt.cm.get_cmap(base_cmap, N)\n",
    "    # The following works for string, None, or a colormap instance:\n",
    "    if base_cmap is None:\n",
    "        base = plt.get_cmap()\n",
    "    else:\n",
    "        base = plt.cm.get_cmap(base_cmap)\n",
    "    \n",
    "    color_list = base(np.linspace(0, 1, N))\n",
    "    cmap_name = base.name + str(N)\n",
    "    return base.from_list(cmap_name, color_list, N)\n",
    "\n",
    "    \n",
    "def histTPM(df,COL='TPM'):\n",
    "    fig,axs= plt.subplots(1,2,figsize=[14,3])\n",
    "    vals = df[COL]\n",
    "    plt.sca(axs[0])\n",
    "    plt.hist(np.log1p(vals),30,log=0)\n",
    "    plt.vlines(1,0,6000)\n",
    "    plt.grid()\n",
    "#     plt.show()\n",
    "\n",
    "    plt.sca(axs[1])\n",
    "    plt.hist(vals,30,log=1)\n",
    "    plt.grid()\n",
    "#     plt.show()\n",
    "    \n",
    "def timePCA(M,ZTime):\n",
    "#     COL = meta[colorName]\n",
    "    COL_RGB,(COL_LAB,COL_LST) = ser2col(pd.Series(ZTime))\n",
    "    \n",
    "    fig,axs= plt.subplots(1,2,figsize=[14,4])\n",
    "\n",
    "    plt.sca(axs[0])    \n",
    "    labs = np.arange(len(M))\n",
    "    x = [int(x.lstrip('ZT')) for x in COL]\n",
    "    y = M[:,0]\n",
    "    plt.xlabel('time')\n",
    "    plt.ylabel('PC1')\n",
    "    l = plt.scatter(x,y,c=COL_RGB,)\n",
    "    for i,(xx,yy,lab) in enumerate(zip(x,y,labs)):\n",
    "        plt.annotate(lab,xy=(xx,yy), )\n",
    "#     plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.sca(axs[1])    \n",
    "    y = M[:,1]\n",
    "    plt.xlabel('time')\n",
    "    plt.ylabel('PC1')\n",
    "    l = plt.scatter(x,y,c=COL_RGB,)\n",
    "    for i,(xx,yy,lab) in enumerate(zip(x,y,labs)):\n",
    "        plt.annotate(lab,xy=(xx,yy), )\n",
    "#     plt.legend()\n",
    "    plt.grid()\n",
    "    return fig    \n",
    "def histoLine(xs,BINS=None,log= 0,**kwargs):\n",
    "    ys,edg = np.histogram(xs,BINS)\n",
    "    ct = (edg[1:] + edg[:-1])/2\n",
    "    if log:\n",
    "        ys = np.log1p(ys)\n",
    "    else:\n",
    "        pass\n",
    "    l =plt.plot(ct,ys,**kwargs)\n",
    "    return l\n",
    "\n",
    "matHist = pyvis.matHist\n",
    "abline = pyvis.abline\n",
    "\n",
    "def qc_2var(xs,ys,xlab='$x$',ylab='$y$',markersize=None,clu=None,xlim=None,ylim=None,axs = None):\n",
    "    ''' Plot histo/scatter/density qc for two variables\n",
    "'''\n",
    "    if axs is None:\n",
    "        fig,axs= plt.subplots(1,3,figsize=[14,3])\n",
    "    xs = np.ravel(xs)\n",
    "    ys = np.ravel(ys)\n",
    "    xlim = xlim if xlim is not None else np.span(xs,99.9)\n",
    "    ylim = ylim if ylim is not None else np.span(ys,99.9)\n",
    "    BX = np.linspace(*xlim, num=30)\n",
    "    BY = np.linspace(*ylim, num=50)\n",
    "#         xlim = np.span(BX)\n",
    "#         ylim = np.span(BY)\n",
    "    if clu is not None:\n",
    "        pass\n",
    "    else:\n",
    "        clu = [0]*len(xs)\n",
    "    clu = np.ravel(clu)\n",
    "    \n",
    "    df = pd.DataFrame({'xs':xs,'ys':ys,'clu':clu})\n",
    "    nMax = 3000\n",
    "    for k, dfc in df.groupby('clu'):\n",
    "        if len(dfc)>nMax:\n",
    "            dfcc = dfc.sample(nMax)\n",
    "        else:\n",
    "            dfcc = dfc\n",
    "#         print k,dfc\n",
    "#         xs,ys,_ = dfcc.values.T\n",
    "        xs,ys = dfcc['xs'].values, dfcc['ys'].values\n",
    "        xs = xs.ravel()\n",
    "        ys = ys.ravel()\n",
    "        \n",
    "        plt.sca(axs[0])\n",
    "        histoLine  (xs,BX,alpha=0.4)    \n",
    "        plt.sca(axs[1])\n",
    "        plt.scatter(xs,ys,markersize,marker='.')\n",
    "        \n",
    "        plt.sca(axs[2])\n",
    "        ct,BX,BY = np.histogram2d(xs, ys,(BX,BY))\n",
    "        plt.pcolormesh(BX,BY,np.log1p(ct).T,)\n",
    "    \n",
    "    plt.sca(axs[0])\n",
    "    plt.grid(1)\n",
    "    plt.xlabel(xlab)\n",
    "    plt.xlim(xlim)\n",
    "\n",
    "    plt.sca(axs[1])\n",
    "    plt.grid(1)\n",
    "    abline()\n",
    "    plt.xlabel(xlab);plt.ylabel(ylab)\n",
    "    plt.xlim(xlim);plt.ylim(ylim)\n",
    "\n",
    "    plt.sca(axs[2])\n",
    "    plt.xlabel(xlab); plt.ylabel(ylab)\n",
    "    return axs\n",
    "    \n",
    "def getCV(xs):\n",
    "    return np.std(xs)/np.mean(xs)\n",
    "\n",
    "def getCol(dfs,COLUMN='Coverage'):\n",
    "    ctraw = [df.get(COLUMN).values for df in dfs]\n",
    "#     ctraw = map(lambda x:getTPM(x,COL=COLUMN),dfs)\n",
    "    ctraw = np.array(ctraw)\n",
    "    return ctraw\n",
    "\n",
    "\n",
    "    \n",
    "def subset(dfs,idx):\n",
    "    return [df.iloc[idx] for df in dfs]\n",
    "\n",
    "def qc_Scatter(x,y,xlab='x',ylab='y',axs = None,bins=(40,40)):\n",
    "    if axs is None:\n",
    "        fig,axs= plt.subplots(1,2,figsize=[14,3])\n",
    "    for v in ['x','y']:\n",
    "        if isinstance(eval(v),pd.Series):\n",
    "            exec('{v}lab={v}.name'.format(v=v))\n",
    "    plt.sca(axs[1])\n",
    "    ct,binx,biny = np.histogram2d(x,y,bins=bins)\n",
    "    plt.pcolormesh(binx,biny, log2p1(ct.T))    \n",
    "\n",
    "    plt.sca(axs[0])\n",
    "    plt.scatter(x,y,2)\n",
    "    plt.xlim(pyutil.np.span(binx))\n",
    "    plt.ylim(pyutil.np.span(biny))\n",
    "    abline()\n",
    "    \n",
    "    R2 = np.corrcoef(x,y)[0,1] ** 2\n",
    "    \n",
    "    for ax in axs:\n",
    "        plt.sca(ax)\n",
    "        plt.grid()\n",
    "        plt.xlabel(xlab)\n",
    "        plt.ylabel(ylab)\n",
    "    plt.suptitle('$R^2=%.4f$'%R2)\n",
    "    return fig,axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### I/O utility\n",
    "\n",
    "def combine_csv(fnames,CUTOFF=6,idCol = 'Gene ID'):\n",
    "    geneSet = set()\n",
    "    dfs = []\n",
    "#     CUTOFF = 6\n",
    "\n",
    "#     FCOL = 'Coverage'\n",
    "    geneAll = set()\n",
    "    geneAny = set()\n",
    "    geneRef = pd.DataFrame()\n",
    "    # for i,fname in enumerate(fnames[:10]):\n",
    "    for i,fname in enumerate(fnames):\n",
    "        if not i%10:\n",
    "            print 'Reading %s'%fname\n",
    "        df = pd.read_table(fname).rename(columns={idCol:'Gene ID'})\n",
    "        allGene = df  \n",
    "        exprGene = allGene\n",
    "#         exprGene = allGene.loc[df[FCOL]>=CUTOFF]    \n",
    "\n",
    "        geneDiff = set(allGene).difference(geneAll)    \n",
    "        appd = df[allGene.isin(geneDiff)]\n",
    "\n",
    "        geneRef = geneRef.append(appd)\n",
    "\n",
    "        geneAll.update(allGene)\n",
    "        geneAny.update(exprGene)\n",
    "    #     break\n",
    "    #     if not geneAll:\n",
    "    #         geneAll.update(df['Gene ID'])\n",
    "    #     else:\n",
    "    #         geneSet.intersection_update(df['Gene ID'])\n",
    "        dfs.append(df)\n",
    "\n",
    "    geneRef.loc[:,['FPKM','TPM']] = 0\n",
    "    geneRef.sort_values('Gene ID',inplace=True)\n",
    "    geneRef = geneRef.reset_index()\n",
    "    geneSet = geneAny\n",
    "    geneValid = geneRef[geneRef['Gene ID'].isin(geneSet)] \n",
    "\n",
    "    print 'Nmber of Genes before filtering:',len(geneAll)\n",
    "    print 'Nmber of Genes after filtering:',len(geneSet)\n",
    "    print 'Surviving rate: ',float(len(geneSet))/len(geneAll)\n",
    "    \n",
    "    return dfs,(geneRef,geneValid)\n",
    "def padWithRef(df,ref,idCol = 'Gene ID'):\n",
    "    df = df.append( ref[~ref[idCol].isin(df[idCol])])\n",
    "    df.sort_values( idCol,inplace=True)\n",
    "    df = df.reset_index()\n",
    "    return df\n",
    "def routine_combineCSV(fnames,CUTOFF=1,idCol='Gene ID'):\n",
    "    print '[PROG] Starting to readfile'\n",
    "    dfs,(geneRef,geneValid) = combine_csv(fnames,CUTOFF=CUTOFF,idCol = idCol)\n",
    "    print '[PROG] Finished to readfile'\n",
    "    \n",
    "    print '[PROG] Starting to pad'\n",
    "    f = pyutil.functools.partial(padWithRef,ref=geneRef)\n",
    "    lst = pyutil.mp_map(f,dfs,n_cpu=1)\n",
    "\n",
    "    SHP = np.array([df.shape for df in lst])\n",
    "    assert np.all(SHP == SHP[0:1]),'Arrays not sharing shape:%s'%SHP\n",
    "    gids = np.array([df['Gene ID'] for df in lst])\n",
    "    assert np.all(gids == gids[0:1])\n",
    "    print '[PROG] Finished padding'\n",
    "    \n",
    "    dfs = lst\n",
    "    dfs = [df.iloc[geneValid.index] for df in dfs]\n",
    "    return dfs,(geneRef,geneValid.reset_index().drop('index',1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(C,std=1):\n",
    "    C = np.log1p(C)\n",
    "#     C = C[clu==1,:][:,meta_wt_LD.index]\n",
    "    C = (C-C.mean(axis=1,keepdims=True))\n",
    "    if std:\n",
    "        STD= C.std(axis=1,keepdims=True)\n",
    "        nonCst = (STD!=0).squeeze()\n",
    "        C[nonCst] = C[nonCst]/STD[nonCst]\n",
    "    return C\n",
    "\n",
    "\n",
    "\n",
    "# def sortLabel(Y,X,#return_pos=0\n",
    "#              ):\n",
    "#     ### Sorting by maximum\n",
    "# #         X = vX\n",
    "#     X  = X-X.mean(axis=1,keepdims=True)\n",
    "#     coord = np.arange(X.shape[-1])[None]\n",
    "#     wt_X = (X == X.max(axis=1,keepdims=True))*coord\n",
    "# #         wt_X = X * coord\n",
    "#     cis = list(range(max(Y)+1))\n",
    "#     pos =  [wt_X[Y == ci,:].mean() for ci in cis]\n",
    "#     sidx = np.argsort(pos)\n",
    "# #     if return_pos:\n",
    "# #         return pos[sidx]\n",
    "# #     else:\n",
    "#     pj = dict(zip(sidx,cis))\n",
    "#     Y = np.vectorize(pj.get)(Y)\n",
    "#     return Y,np.take(pos,sidx)\n",
    "def qcGMM(model,train_data,name='Test',valid_data = None,pt=None,axs = None,**kwargs):\n",
    "    mdl = model\n",
    "    X   = train_data\n",
    "    if valid_data is None:\n",
    "        valid_data = X\n",
    "    vX = valid_data\n",
    "    if axs is None:\n",
    "        fig,axs = plt.subplots(1,2,gridspec_kw={\"width_ratios\": (.1, .9),\n",
    "                                               'wspace':0.1,\n",
    "                                                'top':0.8\n",
    "    #                                             'hspace':0.5\n",
    "                                               },\n",
    "                              figsize=[14,3])\n",
    "    Y = mdl.predict(X)\n",
    "    s = mdl.score_samples(X)\n",
    "    \n",
    "\n",
    "    Y,pos = sortLabel(Y,X)\n",
    "    \n",
    "        \n",
    "    idx = np.argsort(Y)\n",
    "    if pt is not None:\n",
    "        ps = np.percentile(s, pt)\n",
    "        if pt > 50:\n",
    "            i2 = s[idx] > ps\n",
    "        else:\n",
    "            i2 = s[idx] < ps\n",
    "        im = vX[idx[i2]].T\n",
    "    else:\n",
    "        im = vX[idx].T\n",
    "        \n",
    "    ax = axs[0]\n",
    "    plt.sca(axs[0])        \n",
    "    ax.hist(s,50);\n",
    "    if pt is not None:\n",
    "        ax.vlines(ps,0,2000)\n",
    "    plt.grid()\n",
    "\n",
    "#     plt.sca(axs[1])\n",
    "    ax = axs[1]\n",
    "    ax.matshow(im,aspect='auto')\n",
    "    ax.xaxis.tick_bottom()\n",
    "    ax.set_title(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook util.ipynb to python\n",
      "[NbConvertApp] Writing 50325 bytes to util.py\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    !jupyter nbconvert --to python util.ipynb\n",
    "# !python compile_meta.ipynb && echo '[succ]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PCA utilities\n",
    "\n",
    "import pymisca.vis_util as pyvis\n",
    "from synotil.modelRoutine import fit_PCA\n",
    "\n",
    "def quickPCA(trans_data=None,model=None, COL_SER=None,index=None,**kwargs):\n",
    "    M = trans_data\n",
    "    mdl = model\n",
    "    nSample =   len(M)\n",
    "    assert nSample < 100,'Too many samples in the maxtrix: %d>100'%nSample\n",
    "    \n",
    "#     labs = np.arange(nSample)\n",
    "    labs = index\n",
    "    vara = labs * 0. if mdl is None else mdl.explained_variance_ratio_\n",
    "\n",
    "    if not isinstance(COL_SER,pd.Series):\n",
    "        COL_SER = pd.Series(COL_SER)\n",
    "    \n",
    "    COL_RGB,(COL_LAB,COL_LST) = ser2col(COL_SER)\n",
    "    \n",
    "    common = {\n",
    "             }\n",
    "    fig,axs= plt.subplots(1,2,figsize=[14,4])\n",
    "\n",
    "    def pc2d(pi,pj):\n",
    "        x, y  = M[:,pi],M[:,pj]\n",
    "        l = plt.scatter(x,y,c=COL_RGB,)\n",
    "        for _,(xx,yy,lab) in enumerate(zip(x,y,labs)):\n",
    "            plt.annotate(lab,xy=(xx,yy), )\n",
    "    #     plt.legend()\n",
    "        plt.grid()\n",
    "        plt.xlabel('PC%d(%.1f%%)'%(pi+1,vara[pi]*100))\n",
    "        plt.ylabel('PC%d(%.1f%%)'%(pj+1,vara[pj]*100))\n",
    "    plt.sca(axs[0])\n",
    "    pc2d(0,1)\n",
    "    plt.sca(axs[1])\n",
    "    pc2d(2,3)\n",
    "    recs = [pyvis.mpl.patches.Rectangle((0,0),1,1,fc=c) for c in COL_LST]\n",
    "    fig.legend(recs,COL_LAB)    \n",
    "def discrete_cmap(N, base_cmap=None):\n",
    "    \"\"\"Create an N-bin discrete colormap from the specified input map\n",
    "    Source: https://gist.github.com/jakevdp/91077b0cae40f8f8244a\n",
    "    \"\"\"\n",
    "\n",
    "    # Note that if base_cmap is a string or None, you can simply do\n",
    "    #    return plt.cm.get_cmap(base_cmap, N)\n",
    "    # The following works for string, None, or a colormap instance:\n",
    "    if base_cmap is None:\n",
    "        base = plt.get_cmap()\n",
    "    else:\n",
    "        base = plt.cm.get_cmap(base_cmap)\n",
    "    \n",
    "    color_list = base(np.linspace(0, 1, N+1))\n",
    "    cmap_name = base.name + str(N)\n",
    "    return base.from_list(cmap_name, color_list, N+1)\n",
    "\n",
    "def ser2col(COL_SER):\n",
    "    COL_VAL, COL_LAB= COL_SER.factorize()\n",
    "    NCAT = len(COL_LAB)\n",
    "#     cmap = plt.get_cmap('jet')\n",
    "    cmap = discrete_cmap(NCAT,'jet')    \n",
    "#     print (COL_VAL.ravel())\n",
    "    COL_RGB = cmap(COL_VAL.ravel())\n",
    "    COL_LST = cmap(range(NCAT))\n",
    "    return COL_RGB,(COL_LAB,COL_LST)\n",
    "\n",
    "    \n",
    "def timePCA(ZTime_int,trans_data=None, model = None,COL_SER=None,index=None,**kwargs):\n",
    "    mdl = model; M = trans_data\n",
    "#     COL = meta[colorName]\n",
    "    if COL_SER is None:\n",
    "        COL_SER = ZTime_int\n",
    "    COL_RGB,(COL_LAB,COL_LST) = ser2col(pd.Series(COL_SER))\n",
    "    \n",
    "    if index is None:\n",
    "        labs = np.arange(len(M))\n",
    "        print '[WARN] index not specified'\n",
    "    else:\n",
    "        labs = index\n",
    "    vara = labs * 0. if mdl is None else mdl.explained_variance_ratio_\n",
    "        \n",
    "    nPC = 4\n",
    "    fig,axs= plt.subplots(1,nPC,figsize=[14,4])\n",
    "        \n",
    "    for i in range(nPC):\n",
    "        pi = i\n",
    "        plt.sca(axs[i])    \n",
    "        x = ZTime_int\n",
    "        y = M[:,i]\n",
    "        plt.xlabel('ZTime')\n",
    "#         plt.ylabel('PC%d'%(i+1))\n",
    "        plt.ylabel('PC%d(%.1f%%)'%(pi+1,vara[pi]*100))\n",
    "        l = plt.scatter(x,y,c=COL_RGB,)\n",
    "\n",
    "        for i,(xx,yy,lab) in enumerate(zip(x,y,labs)):\n",
    "            plt.annotate(lab,xy=(xx,yy), )\n",
    "    #     plt.legend()\n",
    "        plt.grid()\n",
    "    recs = [pyvis.mpl.patches.Rectangle((0,0),1,1,fc=c) for c in COL_LST]\n",
    "    fig.legend(recs,COL_LAB)    \n",
    "\n",
    "    return fig    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Cannot find file:key.gene\n"
     ]
    }
   ],
   "source": [
    "#### Cluster Profiling Utilities\n",
    "fname = 'key.gene'\n",
    "try:\n",
    "    geneKey = pd.read_table(fname)\n",
    "except:\n",
    "    print \"[WARN] Cannot find file:%s\"%fname\n",
    "    geneKey = None\n",
    "\n",
    "def findMarker(df,concise=1,silent = 0,geneKey=geneKey,how='inner'):\n",
    "    ''' \"geneKey\" needs to have at least 'Gene Name' and 'Bio Name' \n",
    "    '''\n",
    "    import IPython.display as ipd\n",
    "    if not isinstance(df,pd.DataFrame):\n",
    "        df = pd.DataFrame({'Gene Name': df,'Gene ID':df})\n",
    "    df = df.reset_index().merge(geneKey,how=how).set_index('index')\n",
    "    df = df.rename(columns={'Gene ID':'Hit ID',\n",
    "                       'Gene Name':'Query ID'})\n",
    "    if concise:\n",
    "#         df = df[['Hit ID',u'Query ID',u'Bio Name', u'Major role', u'Type of gene']]\n",
    "        df = df[['Hit ID',u'Query ID',u'Bio Name']]\n",
    "#         df = df[['Hit ID',u'Query ID',u'Bio Name', u'Major role', u'Type of gene']]\n",
    "    if not silent:\n",
    "        print '[MARKER] Found %d/%d' %(sum(~df['Hit ID'].isnull()),len(geneKey))\n",
    "        ipd.display(df)\n",
    "    return df\n",
    "\n",
    "def mapTup(lst,n):\n",
    "    res = [x[:n] for x in lst]\n",
    "    return res\n",
    "def isNovo(lst):\n",
    "    res = map(lambda x:x=='STRG',mapTup(lst,4))\n",
    "    return np.array(res)\n",
    "def countNovo(df,):\n",
    "    res = pyutil.collections.Counter([])\n",
    "    return res\n",
    "\n",
    "def meta2name(meta,keys=['gtype','light','Age','ZTime']):\n",
    "    res = pyutil.paste0([meta[k] for k in keys],'_')\n",
    "    return res\n",
    "\n",
    "def qc_GeneExpr(exprMat,idx=None,\n",
    "               gene=None,gRef=None,id_col='Gene Name',\n",
    "                show_ytick = None,\n",
    "               condName=None,**kwargs):\n",
    "    if idx is None:\n",
    "        assert not(gene is None or gRef is None),'Must specify \"gene\" and \"meta\" when \"idx\" not provided'\n",
    "        ### Query dataframe with id\n",
    "        qRes = pyutil.gQuery(gene,gRef,id_col=id_col)\n",
    "        idx = qRes.index\n",
    "    show_ytick = show_ytick or len(idx)<=100\n",
    "    if gene is not None:\n",
    "        ytick = gene.values \n",
    "    elif gRef is not None:\n",
    "        ytick = gRef.loc[idx][id_col]\n",
    "    else:\n",
    "        ytick = idx\n",
    "        print '[WARN] ytick not defined'\n",
    "    \n",
    "    if condName is None:\n",
    "        xtick = None\n",
    "    elif isinstance(condName,pd.DataFrame):\n",
    "        xtick = meta2name(condName)\n",
    "    else:\n",
    "        xtick = condName\n",
    "    ax = pyvis.heatmap(exprMat[idx],\n",
    "                       xlab='Sample ID',ylab='Gene',\n",
    "                       xtick = xtick,\n",
    "                       ytick = ytick if show_ytick else None,**kwargs\n",
    "                      )\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function pymisca.util.packFlat>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pymisca.util as pyutil\n",
    "# pyutil.meta2flat??\n",
    "# pyutil.packFlat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def qc_matrix(C):\n",
    "#     d = pyutil.collections.OrderedDict()\n",
    "#     d['Mean'],d['Std'],d['Shape'] = C.mean(),C.std(),C.shape\n",
    "#     s = '[qc_matrix]%s'% pyutil.packFlat([d.items()],seps=['\\t','='])[0]\n",
    "#     return s\n",
    "    \n",
    "#     (M,V,CV) = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook util.ipynb to python\n",
      "[NbConvertApp] Writing 43452 bytes to util.py\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    !jupyter nbconvert --to python util.ipynb\n",
    "# !python compile_meta.ipynb && echo '[succ]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### patches for BGM model that allows easy reordering of components\n",
    "#### This does not work very well because self._estimate_log_weights() assumes an inherent ordering\n",
    "# reload(pyutil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.mixture as skmix\n",
    "digamma = skmix.bayesian_mixture.digamma\n",
    "def __getitem__(self,ind):\n",
    "#     import pymisca.util as pyutil\n",
    "    getter = pyutil.GitemGetter(ind)\n",
    "    lst = [\n",
    "        'means_',\n",
    "        'mean_precision_',\n",
    "        'covariances_',\n",
    "        'degrees_of_freedom_',\n",
    "        'precisions_',\n",
    "        'precisions_cholesky_',\n",
    "        'weights_',\n",
    "          ]\n",
    "    for k in lst:\n",
    "        val = getattr(self,k)\n",
    "        setattr(self, k, getter(val))\n",
    "    k = 'weight_concentration_'\n",
    "    val = getattr(self,k)\n",
    "    val = tuple(getter(v) for v in val)\n",
    "    setattr(self,k,val)    \n",
    "    \n",
    "    od = self.getorder()\n",
    "    self.order = getter(od)\n",
    "    \n",
    "    return self\n",
    "def getorder(self):\n",
    "    if not hasattr(self,'order'):\n",
    "        self.order =  range(len(self))\n",
    "    else:\n",
    "        self.order = self.order\n",
    "    od = self.order\n",
    "    return od\n",
    "\n",
    "def __len__(self,):\n",
    "    return len(self.means_)\n",
    "\n",
    "def _estimate_log_weights(self):        \n",
    "    if self.weight_concentration_prior_type == 'dirichlet_process':\n",
    "        od = self.getorder()\n",
    "        rod = np.argsort(od)\n",
    "        digamma_sum = digamma(self.weight_concentration_[0] +\n",
    "                              self.weight_concentration_[1])\n",
    "        digamma_a = digamma(self.weight_concentration_[0])\n",
    "        digamma_b = digamma(self.weight_concentration_[1])\n",
    "        return (digamma_a - digamma_sum +\n",
    "                np.hstack((0, \n",
    "                           np.cumsum( (digamma_b - digamma_sum) [rod])[:-1] \n",
    "                          ))[od]\n",
    "               )\n",
    "    else:\n",
    "        # case Variationnal Gaussian mixture with dirichlet distribution\n",
    "        return (digamma(self.weight_concentration_) -\n",
    "                digamma(np.sum(self.weight_concentration_)))\n",
    "    \n",
    "def reorderByMSQ(mdl):\n",
    "    '''Reorder the components of a GMM with a slicing call\n",
    "'''\n",
    "    od = np.sum(mdl.means_**2,axis=1).argsort()[::-1]\n",
    "    mdl = __getitem__(mdl,od.tolist())\n",
    "    return mdl\n",
    "\n",
    "\n",
    "def test_BGM_reorder():\n",
    "# if 1:\n",
    "    import copy\n",
    "    mdl = resA.model\n",
    "    X = resA.values\n",
    "#     od = np.sum(mdl.means_**2,axis=1).argsort().tolist()\n",
    "    \n",
    "#     mm = copy.deepcopy(mdl)[od]\n",
    "    mm = sutil.reorderByMSQ( copy.deepcopy(mdl) )\n",
    "    od1 = np.argsort(mdl.getorder())\n",
    "    od2 = np.argsort(mm.getorder())\n",
    "\n",
    "    v1 = mdl._estimate_log_prob(X=X)\n",
    "    v2 = mm._estimate_log_prob(X)\n",
    "    assert np.all(v1[:,od1]==v2[:,od2])\n",
    "\n",
    "    v1 = mdl._estimate_log_weights()\n",
    "    v2 = mm._estimate_log_weights()\n",
    "    assert np.all(v1[od1]==v2[od2])\n",
    "if __name__ =='__main__':\n",
    "    test_BGM_reorder()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = ['BayesianGaussianMixture','GaussianMixture',]\n",
    "for clsn in lst:\n",
    "    cls = getattr(skmix,clsn)\n",
    "    for mthd in [__getitem__,__len__, reorderByMSQ]:        \n",
    "        setattr(cls,mthd.__name__, mthd)\n",
    "    for mthd in [ _estimate_log_weights, getorder]:\n",
    "        if clsn == 'BayesianGaussianMixture':\n",
    "            setattr(cls,mthd.__name__, mthd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture.gaussian_mixture import *\n",
    "_estimate_gaussian_covariances_diag = skmix.gaussian_mixture._estimate_gaussian_covariances_diag\n",
    "_estimate_gaussian_covariances_tied = skmix.gaussian_mixture._estimate_gaussian_covariances_tied\n",
    "_estimate_gaussian_covariances_full = skmix.gaussian_mixture._estimate_gaussian_covariances_full\n",
    "_estimate_gaussian_covariances_spherical = skmix.gaussian_mixture._estimate_gaussian_covariances_spherical\n",
    "\n",
    "def _estimate_gaussian_parameters(X, resp, reg_covar, covariance_type,fixMean=0):\n",
    "    \"\"\"Estimate the Gaussian distribution parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        The input data array.\n",
    "\n",
    "    resp : array-like, shape (n_samples, n_components)\n",
    "        The responsibilities for each data sample in X.\n",
    "\n",
    "    reg_covar : float\n",
    "        The regularization added to the diagonal of the covariance matrices.\n",
    "\n",
    "    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n",
    "        The type of precision matrices.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nk : array-like, shape (n_components,)\n",
    "        The numbers of data samples in the current components.\n",
    "\n",
    "    means : array-like, shape (n_components, n_features)\n",
    "        The centers of the current components.\n",
    "\n",
    "    covariances : array-like\n",
    "        The covariance matrix of the current components.\n",
    "        The shape depends of the covariance_type.\n",
    "    \"\"\"\n",
    "    nk = resp.sum(axis=0) + 10 * np.finfo(resp.dtype).eps\n",
    "    means = np.dot(resp.T, X) / nk[:, np.newaxis] * (1 - fixMean)\n",
    "    covariances = {\"full\": _estimate_gaussian_covariances_full,\n",
    "                   \"tied\": _estimate_gaussian_covariances_tied,\n",
    "                   \"diag\": _estimate_gaussian_covariances_diag,\n",
    "                   \"spherical\": _estimate_gaussian_covariances_spherical\n",
    "                   }[covariance_type](resp, X, nk, means, reg_covar)\n",
    "    return nk, means, covariances\n",
    "def _m_step(self, X, log_resp):\n",
    "    \"\"\"M step.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "\n",
    "    log_resp : array-like, shape (n_samples, n_components)\n",
    "        Logarithm of the posterior probabilities (or responsibilities) of\n",
    "        the point of each sample in X.\n",
    "    \"\"\"\n",
    "    n_samples, _ = X.shape\n",
    "#     print self.means_\n",
    "    nk, xk, sk = _estimate_gaussian_parameters(\n",
    "        X, np.exp(log_resp), self.reg_covar, self.covariance_type,fixMean=self.fixMean)\n",
    "#     print self.means_\n",
    "    self._estimate_weights(nk)\n",
    "    self._estimate_means(nk, xk)\n",
    "    self._estimate_precisions(nk, xk, sk)\n",
    "skmix.bayesian_mixture._estimate_gaussian_parameters =  _estimate_gaussian_parameters\n",
    "skmix.BayesianGaussianMixture._m_step = _m_step\n",
    "skmix.BayesianGaussianMixture.fixMean = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skmix.GaussianMixture._estimate_log_weights??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook util.ipynb to python\n",
      "[NbConvertApp] Writing 45462 bytes to util.py\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    !jupyter nbconvert --to python util.ipynb\n",
    "# !python compile_meta.ipynb && echo '[succ]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "is in ipython: 1 \n",
      "[WARN]No module named jinja2_util\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] pymisca.vis_util cannot find network\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'modCurr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-905a582fddf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m       ]\n\u001b[1;32m     23\u001b[0m [setattr(modCurr,name,\n\u001b[0;32m---> 24\u001b[0;31m        getattr(norm,name)) for name in lst] \n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodelRoutine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0msubmod\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmodelRoutine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'modCurr' is not defined"
     ]
    }
   ],
   "source": [
    "def normANDproba(mdlDF,X,normF=None):\n",
    "    '''Apply normalisation and then \n",
    "'''\n",
    "    model = mdlDF.model\n",
    "    normF = getattr(sutil,mdlDF.param['normF']) if normF is None else normF\n",
    "    X = X.values if isinstance(X,pd.DataFrame) else X\n",
    "    X = normF(X)\n",
    "    Y = pyutil.predict_proba_safe(model,X)\n",
    "    return Y\n",
    "\n",
    "\n",
    "\n",
    "import norm\n",
    "lst = ['ctNorm',\n",
    "      'identityNorm',\n",
    "       'meanNorm',\n",
    "       'stdNorm',\n",
    "       'meanNormPCA',\n",
    "       'meanNormProj',\n",
    "       'diffNorm',\n",
    "       'sumNorm',\n",
    "      ]\n",
    "[setattr(modCurr,name,\n",
    "       getattr(norm,name)) for name in lst] \n",
    "import modelRoutine\n",
    "submod  = modelRoutine\n",
    "lst= ['fit_BGM']\n",
    "[setattr(modCurr,name,\n",
    "       getattr(submod,name)) for name in lst] \n",
    "\n",
    "\n",
    "# def fit_BGM_AllNorm(C,normLst=None,algoLst=None,ALI='Test',**kwargs):\n",
    "#     if normLst is None:\n",
    "#         normLst = [stdNorm,meanNorm,ctNorm,identityNorm]\n",
    "#     if algoLst is None:\n",
    "#         algoLst = ['DPGMM','DDGMM','GMM',]\n",
    "#     mdls = {}\n",
    "#     for normF in normLst:\n",
    "#         for algo in algoLst:\n",
    "#             mdls[normF.__name__] = fit_BGM(C,normF=normF,\n",
    "#                                            ALI=ALI,\n",
    "#                                            algo = algo,\n",
    "#                                            **kwargs)\n",
    "# #     np.save(ALI,mdls,)        \n",
    "#     return mdls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster as skclu\n",
    "def fit_KMEANS(C,ALI='Test',\n",
    "    maxIt = 1000,\n",
    "    nClu  = 30,\n",
    "    DIR='.',\n",
    "    model_only = 0,\n",
    "    random_state = None,\n",
    "    reorder=0,\n",
    "):\n",
    "    X = C\n",
    "    algo = 'KMEANS'\n",
    "    param = {'genre':algo,\n",
    "            'nClu':nClu,\n",
    "            'maxIt':maxIt,\n",
    "             'randomState':random_state,\n",
    "            }\n",
    "    if not isinstance(X,pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "    param.update(getattr(X,'param',{}))\n",
    "    X,rowName,colName = X.values,X.index,X.columns\n",
    "    \n",
    "    \n",
    "    if ALI =='Test':\n",
    "        ALI = getattr(X,'name','Test')\n",
    "    \n",
    "    mdl = skclu.KMeans(n_clusters=nClu,n_init=1,max_iter=maxIt,\n",
    "                       random_state=random_state)\n",
    "    NAME = '%s_%s'%(ALI, pyutil.dict2flat(param))\n",
    "    \n",
    "    print '[MSG] Now Fitting Model:%s'%NAME\n",
    "    d = {'name': NAME,\n",
    "         'train_data':X,\n",
    "         'colName':colName,\n",
    "         'rowName':rowName,\n",
    "         'param':param,\n",
    "       }    \n",
    "    \n",
    "    try:\n",
    "        logFile = open('%s/%s.log'%(DIR,NAME),'w',0)\n",
    "        with pyutil.RedirectStdStreams(logFile):\n",
    "            mdl.fit(X)\n",
    "            d.update({'suc':1,'model':mdl})\n",
    "        print \"[SUCC] to fit Model:%s\"%(NAME,)\n",
    "    except Exception as e:\n",
    "        print \"[FAIL] to fit Model:%s due to :'%s'\"%(NAME,e)\n",
    "        d.update({'suc':0})\n",
    "    if model_only:\n",
    "        d['train_data'] = None\n",
    "        d['rowName'] = None\n",
    "        d['colName'] = None\n",
    "    \n",
    "\n",
    "    np.save('%s/%s'%(DIR,NAME),d)\n",
    "    d = scount.countMatrix.from_dict(d)\n",
    "    return d    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook util.ipynb to python\n",
      "[NbConvertApp] Writing 53813 bytes to util.py\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    !jupyter nbconvert --to python util.ipynb\n",
    "# !python compile_meta.ipynb && echo '[succ]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_qc_Model(vX,tX=None,normF = None):\n",
    "\n",
    "    ##### Datasets: Training\n",
    "#     vX = None\n",
    "    ##### vX: Validation dataset\n",
    "#     vX = vX[clu==1,:][:,msort[msort['light']=='SD'][msort['Age_int']==2][msort].index]\n",
    "#     vX = util.preprocess(vX,std=1)\n",
    "#     print normF,type(normF)\n",
    "    def qc_Model(#model,train_data,\n",
    "                suc=1,\n",
    "                 name='Test',pt=None,\n",
    "                normF_override=normF,\n",
    "                tX=tX,\n",
    "        **d):\n",
    "        if not suc:\n",
    "            print '[]Skipping failed Model %s'%name\n",
    "            return \n",
    "        print qcmsg.msgGMM(name=name,**d)    \n",
    "        fig,axs = plt.subplots(3,2,gridspec_kw={\"width_ratios\": (.1, .9),\n",
    "                                                   'wspace':0.1,\n",
    "                                                    'hspace':0.5,\n",
    "                                                    'top':0.8\n",
    "                                                   },\n",
    "                                  figsize=[14,6])\n",
    "        normF = normF_override or getattr(modCurr, name.split('_')[0])\n",
    "        if tX is None:\n",
    "            tX = kwargs['train_data']\n",
    "#         if tX is None:\n",
    "#             tX = kwargs['X']\n",
    "        axc= axs[0]\n",
    "        dname = 'Datasets-Training_'\n",
    "        qcGMM(valid_data=normF(tX),pt=pt,axs=axc,name=dname+name,**d)\n",
    "        axc= axs[1]\n",
    "        dname = 'Datasets-Validation_'\n",
    "        qcGMM(valid_data=normF(vX),pt=pt,axs=axc,name=dname+name,**d)\n",
    "        \n",
    "        axc= axs[2]\n",
    "        dname = 'Datasets-ValidMinusTraining'\n",
    "        vD = normF(tX)-normF(vX)\n",
    "#         vD = -(normF(tX)-normF(vX))\n",
    "        qcGMM(valid_data=vD,pt=pt,axs=axc,name=dname+name,**d)\n",
    "        \n",
    "    return qc_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook util.ipynb to python\n",
      "[NbConvertApp] Writing 51319 bytes to util.py\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    !jupyter nbconvert --to python util.ipynb\n",
    "# !python compile_meta.ipynb && echo '[succ]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qc_Sort(df=None,fname=None,cname = 'test',vlim = [-2,2] , title = None,\n",
    "            xlim = None,\n",
    "            ylim = None,\n",
    "            figsize2=[14,6],\n",
    "            nMax = 5000,\n",
    "            **heatargs):\n",
    "    vmin, vmax = vlim\n",
    "    if df is None:\n",
    "        df = pyutil.readData(fname)\n",
    "        if title is None:\n",
    "            title = '[file]%s'%fname\n",
    "    heatargs.update(\n",
    "        {'vmin':vmin,\n",
    "         'vmax':vmax,\n",
    "         'cname':cname,\n",
    "        }    )\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        C = df.values\n",
    "    else:\n",
    "        C = df\n",
    "    (M,V,CV),axsLst = qcAvg(C,silent=0,xlim=xlim,ylim = ylim,nMax=nMax)\n",
    "    plt.suptitle(title)\n",
    "    inter = -len(C)//1000\n",
    "    \n",
    "    fig,axs= plt.subplots(3,1,figsize=figsize2,gridspec_kw={'hspace':0.3})\n",
    "    axs=axs.flat\n",
    "    pyvis.heatmap(C[V.argsort()][::inter],transpose=1,\n",
    "                 main='sorted by Varaince',ax=axs[0],**heatargs)\n",
    "\n",
    "    pyvis.heatmap(C[CV.argsort()][::inter],transpose=1,\n",
    "                 main='sorted by CV',ax=axs[1],**heatargs)\n",
    "\n",
    "    pyvis.heatmap(C[M.argsort()][::inter],transpose=1,\n",
    "                 main='sorted by Average',ax=axs[2],**heatargs)\n",
    "    \n",
    "    axsLst = np.hstack([axsLst,axs])\n",
    "    return (M,V,CV),axsLst\n",
    "\n",
    "\n",
    "def qc_minfo(resA=None,resB=None,\n",
    "             cluA= None,cluB = None,X=None,\n",
    "             CUTOFF=30,xlab=None,ylab=None,maxLine=4,vlim = [-2,2],\n",
    "            silent=1,short=1,):\n",
    "    '''display log-bias matrix\n",
    "'''\n",
    "        \n",
    "    if resA is not None:\n",
    "        ##### Managing your index is crucialllllllllll!\n",
    "        if X is not None:\n",
    "            cluA = resA.model.predict_proba(X)\n",
    "            cluB = resB.model.predict_proba(X)\n",
    "        else:\n",
    "            index =resA.index\n",
    "            resA = resA.reindex(index)\n",
    "            resB = resB.reindex(index)\n",
    "            cluA = resA.model.predict_proba(resA.values)\n",
    "            cluB = resB.model.predict_proba(resB.values)\n",
    "#         N = len(index)\n",
    "    else:\n",
    "        assert cluA is not None\n",
    "    N = len(cluA)\n",
    "    cluA = np.log(cluA)\n",
    "    cluB = np.log(cluB)\n",
    "#     axis = 1\n",
    "#     A = pyutil.get_logP(df = resA ,  axis = axis)\n",
    "#     B = pyutil.get_logP(df = resB ,  axis = axis)\n",
    "    # A = prob2Onehot(A);B=prob2Onehot(B)\n",
    "    # B = A\n",
    "    # A = B\n",
    "\n",
    "    \n",
    "    logC = pyutil.proba_crosstab(cluA,cluB) #### estimate joint distribution of labels\n",
    "    margs =pyutil.get_marginal(logC) #### calculate marginal\n",
    "    entC = pyutil.wipeMarg(logC,margs =margs)      #### wipe marginals from jointDist\n",
    "\n",
    "#     CUTOFF = 30\n",
    "    MI = pyutil.entExpect(logC)\n",
    "    # MI = np.sum(np.exp(logC)*entC)\n",
    "    H1 = -pyutil.entExpect(margs[0])\n",
    "    H2 = -pyutil.entExpect(margs[1])\n",
    "\n",
    "    if not silent:\n",
    "        print 'MI=',MI\n",
    "        print 'H1=',H1\n",
    "        print 'H2=',H2\n",
    "        fig,axs= plt.subplots(1,2,figsize=[14,4]);axs=axs.ravel()\n",
    "        if resA is not None:\n",
    "            xlab = resA.formatName(maxLine=maxLine) if xlab is None else xlab\n",
    "        if resB is not None:\n",
    "            ylab = resB.formatName(maxLine=maxLine) if ylab is None else ylab\n",
    "        \n",
    "        im = entC\n",
    "        if CUTOFF is not None:\n",
    "            xidx = np.where((np.exp(margs[0].ravel())*N)>CUTOFF)[0]\n",
    "            yidx = np.where((np.exp(margs[1].ravel())*N)>CUTOFF)[0]\n",
    "            im = im[xidx][:,yidx]\n",
    "        \n",
    "        pyvis.heatmap(logC,transpose=1,cname='log proba', ax=axs[0])\n",
    "        pyvis.heatmap(im.T,\n",
    "                      vlim=vlim,\n",
    "                      cname='log likelihood ratio',\n",
    "                      ax=axs[1],\n",
    "                      xlab = xlab,\n",
    "                      ylab = ylab,\n",
    "                      ytick=yidx,\n",
    "                      xtick=xidx)\n",
    "    if short:\n",
    "        return [MI,H1,H2]\n",
    "    else:\n",
    "        return [MI,H1,H2], [entC,logC,margs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PanelPlot as spanel\n",
    "def make_interViewer(resA,resB,):\n",
    "    cluA = clu = resA.predict()\n",
    "    cluB = clu = resB.predict()\n",
    "    \n",
    "    tracks = [spanel.fixCluster(cluA),\n",
    "              spanel.fixCluster(cluB), \n",
    "              resA, \n",
    "              resB]    \n",
    "    stats = pd.concat([cluA,cluB],axis=1); \n",
    "\n",
    "    def view_inter(ca,cb,concise=0,\n",
    "                   cluA=cluA,cluB=cluB,tracks=tracks,stats = stats):\n",
    "        indA = cluA.loc[cluA[0] == ca].index\n",
    "        indB = cluB.loc[cluB[0] == cb].index\n",
    "        indAll =indA.intersection(indB)\n",
    "        indAny = indA + indB\n",
    "        if concise==0:\n",
    "            inds = [indAny,indA,indB,indAll,]\n",
    "        elif concise == 1:\n",
    "            inds = [indAll]\n",
    "        elif concise == -1:\n",
    "            inds = [indAny]\n",
    "        print tuple(x.shape for x in inds)\n",
    "        views = []\n",
    "        for index in inds:\n",
    "            pp = spanel.panelPlot(tracks)\n",
    "            # index = df.index\n",
    "    #         index = cluA.loc[cluA[0] == ca].index\n",
    "            pp.compile(index=index,order = stats)\n",
    "            pp.render();\n",
    "            views += [pp]\n",
    "        return views\n",
    "    return view_inter,(tracks,stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook util.ipynb to python\n",
      "[NbConvertApp] Writing 50716 bytes to util.py\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    !jupyter nbconvert --to python util.ipynb\n",
    "# !python compile_meta.ipynb && echo '[succ]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import CountMatrix as scount\n",
    "ctMat = scount\n",
    "# from countMatrix import countMatrix\n",
    "countMatrix = ctMat.countMatrix\n",
    "sortLabel = ctMat.sortLabel\n",
    "# import countMatrix;reload(countMatrix)\n",
    "# from countMatrix import countMatrix\n",
    "\n",
    "def qc_ModelDict(dd=None,fname=None,ali=None,geneKey=None,DIR=None,\n",
    "                 clu = None,cluMax = 100,\n",
    "                 vlim= None\n",
    "                ):\n",
    "    if isinstance(geneKey,dict ):\n",
    "        geneKey = pd.Dataframe.from_dict(geneKey)\n",
    "        geneKey[1] = geneKey.index; \n",
    "        geneKey.rename(columns={0:'Bio Name',\n",
    "                                1:'Gene Name'})\n",
    "    if dd is None:\n",
    "        dd = countMatrix.from_npy(fname)\n",
    "        ali = fname.rsplit('.',1)[0]\n",
    "        \n",
    "    if dd.suc ==0:\n",
    "        print '[WARN] this model is empty due to a failure %s'%dd['name']\n",
    "        return\n",
    "    if vlim is None:\n",
    "        vlim = np.span(dd.train_data,p=99.9)\n",
    "#         geneKey.rename({})\n",
    "    sper = 0\n",
    "#     ali = NBNAME+'_h%d_'%75+\n",
    "    if ali is not None:\n",
    "        ali = ali.rsplit('/',1)[-1]\n",
    "    else:\n",
    "        ali = dd.__dict__.get('name','test')\n",
    "        if isinstance(ali,list):\n",
    "            ali = ':'.join(ali)\n",
    "    DIR = os.path.abspath(DIR or '.')\n",
    "    \n",
    "#     os.system('mkdir -p %s/src'%DIR)\n",
    "#     os.system('mkdir -p %s'%DIR)\n",
    "    print '[ALI]',DIR,'/',ali\n",
    "    \n",
    "    mdl,tX = dd.model,dd.train_data; tXsd = stdNorm(tX)\n",
    "    gRef,condName = dd.rowName,dd.colName_short()\n",
    "    \n",
    "    #### Process rowName\n",
    "    gRef = pd.DataFrame({'Gene Name':gRef,'Gene ID':gRef})\n",
    "    if geneKey is not None:\n",
    "        gRef = findMarker(gRef, geneKey=geneKey,silent=1,how='left',concise=1)\n",
    "        gRef['isMarker']=~gRef['Bio Name'].isnull()\n",
    "        gRef = gRef.rename(columns={'Query ID':'Gene Name'}).drop('Hit ID',1)\n",
    "        print '[GREF]',len(gRef)\n",
    "    \n",
    "    if isinstance(mdl,list):\n",
    "        print dd.nCol\n",
    "        tX = tX[:,:dd.nCol[0]]; nidx = np.isnan(tX[:,0])\n",
    "        mdl = mdl[0]\n",
    "        if any(nidx):        \n",
    "            tX,nX = tX[~nidx],tX[nidx]; nn = sum(nidx)\n",
    "            Y = mdl.predict(tX); s = mdl.score_samples(tX); \n",
    "            Y,pos = sortLabel(Y,tX)\n",
    "            Y = np.hstack([Y,[max(Y)+1]*nn]); s = np.hstack([s,[-1]*nn]); sbin = s> np.percentile(s,sper); \n",
    "            print tX.shape,sbin.shape\n",
    "        else:\n",
    "            Y = mdl.predict(tX); s = mdl.score_samples(tX); sbin = s> np.percentile(s,sper); \n",
    "            Y,pos = sortLabel(Y,tX)\n",
    "        tX = dd.train_data\n",
    "    else:\n",
    "        Y = mdl.predict(tX); s = mdl.score_samples(tX); sbin = s> np.percentile(s,sper); \n",
    "        Y,pos = sortLabel(Y,tX)\n",
    "#     dd.setDF(tX)\n",
    "    \n",
    "    \n",
    "    # pcommon= {}\n",
    "    try:\n",
    "        os.system('mkdir -p %s/%s'%(DIR,ali))\n",
    "#         _ , ali = ali.rsplit('/',1)\n",
    "        CWD= os.getcwd()\n",
    "        _ = os.chdir('%s/%s'%(DIR,ali))\n",
    "        os.system('mkdir -p src/')\n",
    "        OFILE = open('main.md','w')\n",
    "        ExcelFile= pd.ExcelWriter('main.xlsx', engine='xlsxwriter')\n",
    "        with pyutil.RedirectStdStreams(OFILE):\n",
    "#             print dd.param\n",
    "            parDF = dd.param if isinstance(dd.param, list) else [dd.param]\n",
    "            parDF = pd.DataFrame(parDF)\n",
    "#             print '[pAss]'\n",
    "            print '\\n',pyutil.pd2md(parDF)\n",
    "#             for k,v in .items():\n",
    "#                 print '%s:%s\\n'%(k,v)\n",
    "            print 'Directory: %s \\n \\n  Model Name: %s'%(DIR,ali)\n",
    "            print '\\n [.xlsx](main.xlsx)',\n",
    "            print '[.tar.gz](main.tar.gz)',\n",
    "            for clu in range(-1,max(Y)+1):\n",
    "                if clu==cluMax:\n",
    "                    break \n",
    "                fig,axs = plt.subplots(2,1,figsize=[max(7,min(14,len(tX)/3.)),\n",
    "                                                   max(5,min(18,len(tX.T)/1.5))],\n",
    "                                       sharex='all',\n",
    "                                     gridspec_kw={'bottom':0.28,'top':0.8,\n",
    "                                                 'left':0.2}\n",
    "                                     )        \n",
    "                if clu == -1:\n",
    "#                     Y,pos = sortLabel(Y,tX)\n",
    "                    cidx = Y>-1                    \n",
    "                    gCur = gRef[cidx]\n",
    "                    sidx = np.argsort(Y);gCur = gCur.iloc[sidx]\n",
    "                    cluName = 'background.gene'\n",
    "                else:\n",
    "                    cidx = Y==clu \n",
    "                    cidx = cidx & sbin\n",
    "                    gCur = gRef[cidx]\n",
    "                    sidx = np.argsort(s[cidx])[::-1];gCur = gCur.iloc[sidx]\n",
    "                    cluName = 'clu%03d.gene'%(clu)\n",
    "                print '\\n Cluster:%d'%(clu),'\\n','Gene Count:%d'%len(gCur)\n",
    "                if len(gCur)==0:\n",
    "                    continue\n",
    "                    \n",
    "#                 gCur = pd.DataFrame({'Gene Name':gCur})\n",
    "#                 if clu>=0:\n",
    "                if geneKey is not None:\n",
    "                    gMark = gCur[gCur['isMarker']==1]\n",
    "#                     gMark = findMarker(gCur['Gene Name'],geneKey=geneKey,silent=1,how='right')\n",
    "                    gMark.to_excel(ExcelFile,cluName,index=True,startcol=2)       \n",
    "                    print '\\n',pyutil.pd2md(gMark)\n",
    "#                     gCur = gMark\n",
    "#                     gCur = gMark.rename(columns={'Query ID':'Gene Name'})                \n",
    "                gCur[['Gene Name']].to_csv('src/%s'%cluName,index=0,)\n",
    "#                 gCur[['Gene Name']].to_excel(ExcelFile,cluName,index=True,)                \n",
    "                dd.df.loc[gCur['Gene Name']].to_excel(ExcelFile,cluName,index=True,)                \n",
    "                \n",
    "                figList = []\n",
    "                matDict ={'raw':tX,'stdNorm':tXsd}\n",
    "                sheet_curr = ExcelFile.sheets[cluName]\n",
    "                for i,k in enumerate(['raw','stdNorm']):\n",
    "                    C = matDict[k]; ax =axs[i]                    \n",
    "                    im = pyvis.heatmap(C[cidx][sidx],\n",
    "                                       ylab=(None if not i else 'Gene'),\n",
    "                                       ytick = (None if not i else gCur['Gene Name']),\n",
    "                                       xlab='Condition',xtick=condName,transpose=1,\n",
    "                                      vmin=vlim[0],vmax=vlim[1],\n",
    "                                      ax=ax\n",
    "                                      )  \n",
    "                    dd.addBox(ax=ax)\n",
    "#                     figList +=[FFname]\n",
    "                    \n",
    "                    plt.colorbar(im)\n",
    "                    plt.title(k)\n",
    "                plt.suptitle('Cluster %d'%clu,y=1)\n",
    "#                 try:\n",
    "#                 fig.tight_layout()\n",
    "#                 except:\n",
    "#                     print '\\n \\[WARN\\] tight_layout() failed, legend may not display properly'\n",
    "#                     pass                \n",
    "                FFname = 'src/clu%03d.png'%(clu,)\n",
    "                FigMd = pyutil.showsavefig(fname=FFname)\n",
    "                print '\\n',(FigMd) ## remove directory name                \n",
    "                sheet_curr.insert_image(0, 7, FFname)                \n",
    "                plt.show()\n",
    "                plt.close()\n",
    "        \n",
    "        ExcelFile.save()\n",
    "        ExcelFile.close()\n",
    "        os.system('pdext {fname} html'.format(fname=OFILE.name) )\n",
    "        s = '[{0}]({0}/)'.format(dd.name)\n",
    "        if pyutil.hasIPD:\n",
    "            pyutil.ipd.display(pyutil.ipd.Markdown(s))\n",
    "        else:\n",
    "            print s\n",
    "    except Exception as e:\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        print(exc_type, fname, exc_tb.tb_lineno)\n",
    "        raise e\n",
    "    finally:\n",
    "        os.chdir(CWD)\n",
    "#     os.chdir(sys.path[0])\n",
    "#     os.system('pdext {ali}.md pdf'.format(ali=ali) )\n",
    "def log2p1(x):\n",
    "    res = np.log2(x+1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readModels(DIR):\n",
    "    DIR = DIR.rstrip('/')\n",
    "    fnames = pyutil.shellexec(\"find %s/*randomState*.npy | grep normF\"%DIR\n",
    "                         ).splitlines()\n",
    "\n",
    "    res = map(scount.countMatrix.from_npy,fnames)\n",
    "    meta= pyutil.flat2meta([x.replace('/','_').rsplit('.',1)[0] for x in fnames])\n",
    "    meta = pd.DataFrame(map(lambda x:dict([y for y in x if len(y)==2]),meta))\n",
    "    meta['fname_'] = list(fnames) \n",
    "    meta['obj'] = res\n",
    "#     meta.model = [x.model for x in res]  \n",
    "#     meta['model'] = [[x.model] for x in res]  \n",
    "#     meta['model'] = [x.model for x in res]\n",
    "\n",
    "    meta_model = meta\n",
    "    return meta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ptn\n",
    "#### DataSets Management\n",
    "def dfContrast(dfRef,dfObs):\n",
    "    ''' Contrast two DataFrames\n",
    "    '''\n",
    "    C = dfObs.values - dfRef.values\n",
    "    df = pd.DataFrame(C); df.set_index(dfObs.index,inplace=True)\n",
    "    df.columns = pyutil.metaContrast(dfRef.columns,dfObs.columns)\n",
    "    return df\n",
    "\n",
    "def tidyBd(C1,match = 'Brad', ):\n",
    "    if not isinstance(C1,ctMat.countMatrix):\n",
    "        C1 = ctMat.countMatrix.from_DataFrame(C1)\n",
    "    C1 = C1.fillna(0)\n",
    "    if match is not None:\n",
    "        C1 = C1.filterMatch(match)\n",
    "    C1.sanitiseIndex(ptn.BdAcc)\n",
    "    C1.index.name = 'GeneAcc'\n",
    "    return C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook util.ipynb to python\n",
      "[NbConvertApp] Writing 51303 bytes to util.py\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    !jupyter nbconvert --to python util.ipynb\n",
    "# !python compile_meta.ipynb && echo '[succ]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook util.ipynb to python\n",
      "[NbConvertApp] Writing 50984 bytes to util.py\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    !jupyter nbconvert --to python util.ipynb\n",
    "# !python compile_meta.ipynb && echo '[succ]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findPromoter(\n",
    "    INFILE = None,\n",
    "    upStream=1000,\n",
    "    downStream=500 ,\n",
    "    opt = '-s -i -',\n",
    "    filterKey = 'CDS',\n",
    "    OFILE = None,\n",
    "    inplace = 0,\n",
    "    GSIZE = None,\n",
    "    silent=1,\n",
    "):\n",
    "    '''Find the promoter from a GTF file\n",
    "'''\n",
    "    if GSIZE is None:        \n",
    "        TRY = os.environ.get('GSIZE',None)\n",
    "        assert TRY is not None, 'Please specify chromosizes'\n",
    "        GSIZE = TRY\n",
    "    assert os.path.exists(GSIZE),'File does not exist:\"%s\"'%GSIZE\n",
    "    \n",
    "    if OFILE is None:        \n",
    "        OFILE = os.path.basename(INFILE)+'.promoter'\n",
    "    if inplace:\n",
    "        OFILE = os.path.join(os.path.dirname(INFILE),OFILE)\n",
    "\n",
    "    cmd = 'cat %s'%INFILE\n",
    "    if filterKey is not None:\n",
    "        cmd += '| grep {} \\\\\\n'.format(filterKey)\n",
    "    cmd += r'''\n",
    "    | bedtools slop -s -l 0 -r -1.0 -pct {opt} \\\n",
    "    | bedtools slop -s -l {upStream} -r {downStream} {opt} \\\n",
    "    | sed \"s/\\\"//g\"  \\\n",
    "    >{OFILE}\n",
    "    '''.format(\n",
    "#         INFILE = INFILE,\n",
    "        OFILE = OFILE,\n",
    "#         filterKey=filterKey,\n",
    "        upStream = upStream,\n",
    "        downStream = downStream,\n",
    "        opt='%s -g %s'%(opt,GSIZE) ,    \n",
    "    ).strip()\n",
    "    res = pyutil.shellexec(cmd,silent=silent)\n",
    "    return OFILE\n",
    "# %time findPromoter(INFILE='./Bdistachyon_314_v3.1.gene_exons.gtf.cds',inplace=True)\n",
    "# sutil.extractBigWig = extractBigWig\n",
    "\n",
    "\n",
    "\n",
    "# def parseBedmap(df = None, fname = None):\n",
    "#     ''' Parse the output of bedMap\n",
    "# '''\n",
    "#     if df is None:\n",
    "#         df = pd.read_table(fname,header = None)\n",
    "\n",
    "#     df = df.dropna()\n",
    "    \n",
    "#     df.columns = bedHeader + ['hit']\n",
    "\n",
    "#     res = pyutil.explode(df,'hit','acc',';')\n",
    "#     res = res.merge(df.drop('hit',1),on='acc')\n",
    "#     return res\n",
    "\n",
    "# def parseBedClosest(df = None, fname = None):\n",
    "#     ''' Parse the output of 'bedtools closest'\n",
    "# '''\n",
    "#     if df is None:\n",
    "#         df = pd.read_table(fname,header = None,index_col = None)\n",
    "# #     df = df.dropna()    \n",
    "\n",
    "#     header = bedHeader + pyutil.paste0([['feature_'], bedHeader]).tolist()\n",
    "#     df = df.iloc[:,:18]\n",
    "#     df.columns = header[:17] + ['distance']\n",
    "#     df['hit'] = df['feature_acc']\n",
    "#     return df\n",
    "# parseBedClosest = extract_closest \n",
    "\n",
    "\n",
    "import StringIO\n",
    "def closestAnnotation(\n",
    "    bedFile,\n",
    "    RANGE = 1000,\n",
    "    ANNOTATION_FILE=None,\n",
    "    GSIZE=None,\n",
    "    silent = True,\n",
    "):\n",
    "    '''\n",
    "    use bedtools to find the feature closest to the \n",
    "regions contianed inthe in the given bed file.\n",
    "    The annotation will be expanded by {RANGE} bp before queryed\n",
    "    chrom.sizes must be supplied as {GSIZE} to make bedtools happy\n",
    "'''\n",
    "\n",
    "    FOUT= bedFile.split('/')[-1] \n",
    "    FOUT = 'type=closest_bed=%s_feat=%s.tsv'%(pyutil.basename(bedFile),\n",
    "                            pyutil.basename(ANNOTATION_FILE))\n",
    "    cmd = '''\n",
    "bedtools slop -b {RANGE} -i {ANNO} -g {GSIZE} |bedtools sort > {ANNOBASE}.{RANGE}\n",
    "bedtools sort -i {bedFile} |\\\n",
    "bedtools closest -d -a - -b {ANNOBASE}.{RANGE} | tee {FOUT}.tmp\n",
    "'''.format(\n",
    "        GSIZE   = GSIZE,\n",
    "        ANNO    = ANNOTATION_FILE,\n",
    "        ANNOBASE= ANNOTATION_FILE.split('/')[-1],\n",
    "        bedFile = bedFile,\n",
    "        RANGE   = RANGE,\n",
    "        FOUT    = FOUT,\n",
    "    ).strip()\n",
    "    buf = StringIO.StringIO(pyutil.shellexec(cmd,silent=silent))\n",
    "    if buf.len:\n",
    "        buf.seek(0)      \n",
    "        header = sum( [guessBedHeader(x,prefix=k) for k,x in \n",
    "                       [('',bedFile),\n",
    "                        ('feat',ANNOTATION_FILE)]\n",
    "                      ],\n",
    "                     [])\n",
    "        header += ['distance',]\n",
    "        df = pyutil.readData( buf,header = None,ext='tsv',guess_index=False)\n",
    "        df.columns = header\n",
    "#         df = parseBedClosest(fname = buf)\n",
    "#         os.system('rm %s.tmp' % FOUT)\n",
    "    else:\n",
    "        assert 0,' Buffer is empty, check error msg' \n",
    "    df = df[df['distance']==0]\n",
    "    df.to_csv(FOUT,sep='\\t',index=0)\n",
    "    return FOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dataFrame headers \n",
    "from dio import *\n",
    "\n",
    "####\n",
    "from qcplots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook util.ipynb to python\n",
      "[NbConvertApp] Writing 50875 bytes to util.py\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    !jupyter nbconvert --to python util.ipynb\n",
    "# !python compile_meta.ipynb && echo '[succ]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        tmp.set_index( 'ind', append=1,inplace=True)\r\n",
      "        tmp = tmp.reorder_levels(['ind',None])        \r\n",
      "        tmp.index.names = ['bwFile','pos']\r\n",
      "        out = tmp.T\r\n",
      "#     out = ctMat.countMatrix.from_DataFrame(df=out)\r\n",
      "#     out.fname = bwFile\r\n",
      "    out.param = {}\r\n",
      "    out.param['bwFile'] = bwFile\r\n",
      "    out.param['bedFile'] = bedFile\r\n",
      "    return out\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    ! sed -n '1490,1500p' < util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['001', '003', '004', '005']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(['001','005','003','004'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            df.set_index(index_col,drop=0,inplace=True)\r\n",
      "        self.drop(self.columns, 1, inplace=True)\r\n",
      "#     def fillna(self,fill,inplace=True,**kwargs):\r\n",
      "#         pd.DataFrame.fillna(self,fill,inplace=inplace,**kwargs)\r\n",
      "        res = self.set_index([res],inplace=True)\r\n"
     ]
    }
   ],
   "source": [
    "# !grep -e \"inplace\" ./CountMatrix.py.tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting replace.sh\n"
     ]
    }
   ],
   "source": [
    "# %%writefile replace.sh\n",
    "# cat $1 \\\n",
    "#   |sed -e \"s/inplace\\=1/inplace\\=True/g\" \\\n",
    "#   |sed -e \"s/inplace\\=0/inplace\\=False/g\" \\\n",
    "#   >$1.tmp\n",
    "# mv $1.tmp -t $1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: ./.~util.ipynb: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# ! chmod +x replace.sh\n",
    "# ! find . -name \"*.py\" -or -name \"*.ipynb\" | parallel --gnu ./replace.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
